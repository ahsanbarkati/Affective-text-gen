{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AffectiveTextGen.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0439cb92092049b4a14c3cf1b636b5c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [
              "widget-interact"
            ],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_167e4351bff74792bea317b34d0156c1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_219fb0854b5c4dd195f3630fe7ce607a",
              "IPY_MODEL_47b6d4a4d95a4854887aa45e9446625c",
              "IPY_MODEL_9c952f336a3045da9ce304c8da4f81c5",
              "IPY_MODEL_ea6f75d7dae64f1f88f64ee7a2a75595",
              "IPY_MODEL_5f3865f5942e4f4596059a684566d280",
              "IPY_MODEL_881f6e8cf2cb4518a02e4fe3ae96f260"
            ]
          }
        },
        "167e4351bff74792bea317b34d0156c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "219fb0854b5c4dd195f3630fe7ce607a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatSliderModel",
          "state": {
            "_view_name": "FloatSliderView",
            "style": "IPY_MODEL_237ea3c12e9f43489f6b3279f8e0b8a0",
            "_dom_classes": [],
            "description": "Knob",
            "step": 0.1,
            "_model_name": "FloatSliderModel",
            "orientation": "horizontal",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "min": 0,
            "continuous_update": true,
            "readout_format": ".2f",
            "description_tooltip": null,
            "readout": true,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ddb813c3880c41ca8db7dc5acf60e696"
          }
        },
        "47b6d4a4d95a4854887aa45e9446625c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "TextModel",
          "state": {
            "_view_name": "TextView",
            "style": "IPY_MODEL_aba9e6e67ab5489abf867c1ebe855250",
            "_dom_classes": [],
            "description": "Prompt",
            "_model_name": "TextModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "There exists",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "continuous_update": true,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1251f7681af64d1695c4fff67f548863"
          }
        },
        "9c952f336a3045da9ce304c8da4f81c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "state": {
            "_options_labels": [
              "legal",
              "military",
              "monsters",
              "politics",
              "positive_words",
              "religion",
              "science",
              "space",
              "technology"
            ],
            "_view_name": "DropdownView",
            "style": "IPY_MODEL_eb5d3837f49045fa809bdfb7fe7236c1",
            "_dom_classes": [],
            "description": "Topic",
            "_model_name": "DropdownModel",
            "index": 0,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c350528a586b4720820c6b77aa250470"
          }
        },
        "ea6f75d7dae64f1f88f64ee7a2a75595": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "state": {
            "_options_labels": [
              "fear",
              "joy",
              "anger",
              "sadness",
              "anticipation",
              "disgust",
              "surprise",
              "trust"
            ],
            "_view_name": "DropdownView",
            "style": "IPY_MODEL_31e5dd27337740148248d2b82a49377a",
            "_dom_classes": [],
            "description": "Affect",
            "_model_name": "DropdownModel",
            "index": 0,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c7a1f3aeb10949c9899ff47b59cfd1b9"
          }
        },
        "5f3865f5942e4f4596059a684566d280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "state": {
            "_view_name": "ButtonView",
            "style": "IPY_MODEL_869e4090b8eb4036a4dbb4acc8606ee4",
            "_dom_classes": [],
            "description": "Run Interact",
            "_model_name": "ButtonModel",
            "button_style": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "tooltip": "",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_13ded88e4bce422b99a213975c22a679",
            "_model_module": "@jupyter-widgets/controls",
            "icon": ""
          }
        },
        "881f6e8cf2cb4518a02e4fe3ae96f260": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "state": {
            "_view_name": "OutputView",
            "msg_id": "",
            "_dom_classes": [],
            "_model_name": "OutputModel",
            "outputs": [],
            "_view_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_view_count": null,
            "_view_module_version": "1.0.0",
            "layout": "IPY_MODEL_db3b0ac319c940119df64fdeae233b5e",
            "_model_module": "@jupyter-widgets/output"
          }
        },
        "237ea3c12e9f43489f6b3279f8e0b8a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "state": {
            "_view_name": "StyleView",
            "handle_color": null,
            "_model_name": "SliderStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ddb813c3880c41ca8db7dc5acf60e696": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "aba9e6e67ab5489abf867c1ebe855250": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1251f7681af64d1695c4fff67f548863": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eb5d3837f49045fa809bdfb7fe7236c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c350528a586b4720820c6b77aa250470": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "31e5dd27337740148248d2b82a49377a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c7a1f3aeb10949c9899ff47b59cfd1b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "869e4090b8eb4036a4dbb4acc8606ee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ButtonStyleModel",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "button_color": null,
            "font_weight": "",
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "13ded88e4bce422b99a213975c22a679": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "db3b0ac319c940119df64fdeae233b5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishikasingh/Affective-text-gen/blob/master/AffectiveTextGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIEkru9a89ev",
        "outputId": "48703456-310b-4984-b3cd-9c976261c4dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "#### Code based on PPLM framework ####\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.3.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.91)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8.1rc2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09MUYiLt9wta"
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "from operator import add\n",
        "from typing import List, Optional, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from tqdm import trange\n",
        "from transformers import GPT2Tokenizer\n",
        "from transformers.file_utils import cached_path\n",
        "from transformers.modeling_gpt2 import GPT2LMHeadModel\n",
        "from __future__ import print_function\n",
        "from ipywidgets import interact, interactive, fixed, interact_manual\n",
        "import ipywidgets as widgets"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1TvKXs09QVG"
      },
      "source": [
        "class ClassificationHead(torch.nn.Module):\n",
        "    \"\"\"Classification Head for  transformer encoders\"\"\"\n",
        "\n",
        "    def __init__(self, class_size, embed_size):\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.class_size = class_size\n",
        "        self.embed_size = embed_size\n",
        "        # self.mlp1 = torch.nn.Linear(embed_size, embed_size)\n",
        "        # self.mlp2 = (torch.nn.Linear(embed_size, class_size))\n",
        "        self.mlp = torch.nn.Linear(embed_size, class_size)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        # hidden_state = F.relu(self.mlp1(hidden_state))\n",
        "        # hidden_state = self.mlp2(hidden_state)\n",
        "        logits = self.mlp(hidden_state)\n",
        "        return logits"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX4W7SCz94Q9"
      },
      "source": [
        "PPLM_BOW = 1\n",
        "PPLM_DISCRIM = 2\n",
        "PPLM_BOW_DISCRIM = 3\n",
        "BOW_AFFECT = 4\n",
        "SMALL_CONST = 1e-15\n",
        "BIG_CONST = 1e10\n",
        "\n",
        "QUIET = 0\n",
        "REGULAR = 1\n",
        "VERBOSE = 2\n",
        "VERY_VERBOSE = 3\n",
        "VERBOSITY_LEVELS = {\n",
        "    'quiet': QUIET,\n",
        "    'regular': REGULAR,\n",
        "    'verbose': VERBOSE,\n",
        "    'very_verbose': VERY_VERBOSE,\n",
        "}\n",
        "\n",
        "BAG_OF_WORDS_ARCHIVE_MAP = {\n",
        "    'legal': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/legal.txt\",\n",
        "    'military': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/military.txt\",\n",
        "    'monsters': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/monsters.txt\",\n",
        "    'politics': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/politics.txt\",\n",
        "    'positive_words': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/positive_words.txt\",\n",
        "    'religion': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/religion.txt\",\n",
        "    'science': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/science.txt\",\n",
        "    'space': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/space.txt\",\n",
        "    'technology': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/technology.txt\",\n",
        "}\n",
        "\n",
        "DISCRIMINATOR_MODELS_PARAMS = {\n",
        "    \"clickbait\": {\n",
        "        \"url\": \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/discriminators/clickbait_classifier_head.pt\",\n",
        "        \"class_size\": 2,\n",
        "        \"embed_size\": 1024,\n",
        "        \"class_vocab\": {\"non_clickbait\": 0, \"clickbait\": 1},\n",
        "        \"default_class\": 1,\n",
        "        \"pretrained_model\": \"gpt2-medium\",\n",
        "    },\n",
        "    \"sentiment\": {\n",
        "        \"url\": \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/discriminators/SST_classifier_head.pt\",\n",
        "        \"class_size\": 5,\n",
        "        \"embed_size\": 1024,\n",
        "        \"class_vocab\": {\"very_positive\": 2, \"very_negative\": 3},\n",
        "        \"default_class\": 3,\n",
        "        \"pretrained_model\": \"gpt2-medium\",\n",
        "    },\n",
        "}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "503RMAOx976u"
      },
      "source": [
        "def to_var(x, requires_grad=False, volatile=False, device='cuda'):\n",
        "    if torch.cuda.is_available() and device == 'cuda':\n",
        "        x = x.cuda()\n",
        "    elif device != 'cuda':\n",
        "        x = x.to(device)\n",
        "    return Variable(x, requires_grad=requires_grad, volatile=volatile)\n",
        "\n",
        "\n",
        "def top_k_filter(logits, k, probs=False):\n",
        "    \"\"\"\n",
        "    Masks everything but the k top entries as -infinity (1e10).\n",
        "    Used to mask logits such that e^-infinity -> 0 won't contribute to the\n",
        "    sum of the denominator.\n",
        "    \"\"\"\n",
        "    if k == 0:\n",
        "        return logits\n",
        "    else:\n",
        "        values = torch.topk(logits, k)[0]\n",
        "        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n",
        "        if probs:\n",
        "            return torch.where(logits < batch_mins,\n",
        "                               torch.ones_like(logits) * 0.0, logits)\n",
        "        return torch.where(logits < batch_mins,\n",
        "                           torch.ones_like(logits) * -BIG_CONST,\n",
        "                           logits)\n",
        "\n",
        "def gaussian(x, mu, sig):\n",
        "  x = np.array(x)\n",
        "  return list(np.exp(-0.5*((x-mu)/sig)**2)/(sig*(2*np.pi)**0.5))\n",
        "\n",
        "def perturb_past(\n",
        "        past,\n",
        "        model,\n",
        "        last,\n",
        "        affect_weight=0.2,\n",
        "        unpert_past=None,\n",
        "        unpert_logits=None,\n",
        "        accumulated_hidden=None,\n",
        "        grad_norms=None,\n",
        "        stepsize=0.01,\n",
        "        one_hot_bows_vectors=None,\n",
        "        one_hot_bows_affect=None,\n",
        "        affect_int = None,\n",
        "        knob = None,\n",
        "        classifier=None,\n",
        "        class_label=None,\n",
        "        loss_type=0,\n",
        "        num_iterations=3,\n",
        "        horizon_length=1,\n",
        "        window_length=0,\n",
        "        decay=False,\n",
        "        gamma=1.5,\n",
        "        kl_scale=0.01,\n",
        "        device='cuda',\n",
        "        verbosity_level=REGULAR\n",
        "):\n",
        "    # Generate inital perturbed past\n",
        "    grad_accumulator = [\n",
        "        (np.zeros(p.shape).astype(\"float32\"))\n",
        "        for p in past\n",
        "    ]\n",
        "    \n",
        "    if accumulated_hidden is None:\n",
        "        accumulated_hidden = 0\n",
        "\n",
        "    if decay:\n",
        "        decay_mask = torch.arange(\n",
        "            0.,\n",
        "            1.0 + SMALL_CONST,\n",
        "            1.0 / (window_length)\n",
        "        )[1:]\n",
        "    else:\n",
        "        decay_mask = 1.0\n",
        "\n",
        "    # TODO fix this comment (SUMANTH)\n",
        "    # Generate a mask is gradient perturbated is based on a past window\n",
        "    _, _, _, curr_length, _ = past[0].shape\n",
        "\n",
        "    if curr_length > window_length and window_length > 0:\n",
        "        ones_key_val_shape = (\n",
        "                tuple(past[0].shape[:-2])\n",
        "                + tuple([window_length])\n",
        "                + tuple(past[0].shape[-1:])\n",
        "        )\n",
        "\n",
        "        zeros_key_val_shape = (\n",
        "                tuple(past[0].shape[:-2])\n",
        "                + tuple([curr_length - window_length])\n",
        "                + tuple(past[0].shape[-1:])\n",
        "        )\n",
        "\n",
        "        ones_mask = torch.ones(ones_key_val_shape)\n",
        "        ones_mask = decay_mask * ones_mask.permute(0, 1, 2, 4, 3)\n",
        "        ones_mask = ones_mask.permute(0, 1, 2, 4, 3)\n",
        "\n",
        "        window_mask = torch.cat(\n",
        "            (ones_mask, torch.zeros(zeros_key_val_shape)),\n",
        "            dim=-2\n",
        "        ).to(device)\n",
        "    else:\n",
        "        window_mask = torch.ones_like(past[0]).to(device)\n",
        "\n",
        "    # accumulate perturbations for num_iterations\n",
        "    loss_per_iter = []\n",
        "    new_accumulated_hidden = None\n",
        "    for i in range(num_iterations):\n",
        "        if verbosity_level >= VERBOSE:\n",
        "            print(\"Iteration \", i + 1)\n",
        "        curr_perturbation = [\n",
        "            to_var(torch.from_numpy(p_), requires_grad=True, device=device)\n",
        "            for p_ in grad_accumulator\n",
        "        ]\n",
        "\n",
        "        # Compute hidden using perturbed past\n",
        "        perturbed_past = list(map(add, past, curr_perturbation))\n",
        "        _, _, _, curr_length, _ = curr_perturbation[0].shape\n",
        "        all_logits, _, all_hidden = model(last, past=perturbed_past)\n",
        "        hidden = all_hidden[-1]\n",
        "        new_accumulated_hidden = accumulated_hidden + torch.sum(\n",
        "            hidden,\n",
        "            dim=1\n",
        "        ).detach()\n",
        "        # TODO: Check the layer-norm consistency of this with trained discriminator (Sumanth)\n",
        "        logits = all_logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        loss = 0.0\n",
        "        loss_list = []\n",
        "        if loss_type == PPLM_BOW or loss_type == BOW_AFFECT:\n",
        "            for one_hot_bow in one_hot_bows_vectors:\n",
        "                bow_logits = torch.mm(probs, torch.t(one_hot_bow))\n",
        "                #print(type(bow_logits))\n",
        "                bow_loss = -torch.log(torch.sum(bow_logits))\n",
        "                #print(bow_loss)\n",
        "                loss +=  bow_loss\n",
        "                loss_list.append(bow_loss)\n",
        "            if loss_type == BOW_AFFECT:\n",
        "              for one_hot_bow in one_hot_bows_affect:\n",
        "                  bow_logits = torch.mm(probs, torch.t(one_hot_bow))\n",
        "                 # print(bow_logits.size(), torch.FloatTensor(affect_int).size())\n",
        "                  bow_loss = -torch.log(torch.matmul(bow_logits, torch.t(torch.FloatTensor(gaussian(affect_int, knob, .1)).to(device))))#-torch.log(torch.sum(bow_logits))#\n",
        "                  # print(bow_loss)\n",
        "\n",
        "                  loss += affect_weight * bow_loss[0]\n",
        "                  loss_list.append(bow_loss)\n",
        "            if verbosity_level >= VERY_VERBOSE:\n",
        "                print(\" pplm_bow_loss:\", loss.data.cpu().numpy())\n",
        "\n",
        "        kl_loss = 0.0\n",
        "        if kl_scale > 0.0:\n",
        "            unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\n",
        "            unpert_probs = (\n",
        "                    unpert_probs + SMALL_CONST *\n",
        "                    (unpert_probs <= SMALL_CONST).float().to(device).detach()\n",
        "            )\n",
        "            correction = SMALL_CONST * (probs <= SMALL_CONST).float().to(\n",
        "                device).detach()\n",
        "            corrected_probs = probs + correction.detach()\n",
        "            kl_loss = kl_scale * (\n",
        "                (corrected_probs * (corrected_probs / unpert_probs).log()).sum()\n",
        "            )\n",
        "            if verbosity_level >= VERY_VERBOSE:\n",
        "                print(' kl_loss', kl_loss.data.cpu().numpy())\n",
        "            loss += kl_loss\n",
        "\n",
        "        loss_per_iter.append(loss.data.cpu().numpy())\n",
        "        if verbosity_level >= VERBOSE:\n",
        "            print(' pplm_loss', (loss - kl_loss).data.cpu().numpy())\n",
        "\n",
        "        # compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # calculate gradient norms\n",
        "        if grad_norms is not None and loss_type == PPLM_BOW:\n",
        "            grad_norms = [\n",
        "                torch.max(grad_norms[index], torch.norm(p_.grad * window_mask))\n",
        "                for index, p_ in enumerate(curr_perturbation)\n",
        "            ]\n",
        "        else:\n",
        "            grad_norms = [\n",
        "                (torch.norm(p_.grad * window_mask) + SMALL_CONST)\n",
        "                for index, p_ in enumerate(curr_perturbation)\n",
        "            ]\n",
        "\n",
        "        # normalize gradients\n",
        "        grad = [\n",
        "            -stepsize *\n",
        "            (p_.grad * window_mask / grad_norms[\n",
        "                index] ** gamma).data.cpu().numpy()\n",
        "            for index, p_ in enumerate(curr_perturbation)\n",
        "        ]\n",
        "\n",
        "        # accumulate gradient\n",
        "        grad_accumulator = list(map(add, grad, grad_accumulator))\n",
        "\n",
        "        # reset gradients, just to make sure\n",
        "        for p_ in curr_perturbation:\n",
        "            p_.grad.data.zero_()\n",
        "\n",
        "        # removing past from the graph\n",
        "        new_past = []\n",
        "        for p_ in past:\n",
        "            new_past.append(p_.detach())\n",
        "        past = new_past\n",
        "\n",
        "    # apply the accumulated perturbations to the past\n",
        "    grad_accumulator = [\n",
        "        to_var(torch.from_numpy(p_), requires_grad=True, device=device)\n",
        "        for p_ in grad_accumulator\n",
        "    ]\n",
        "    pert_past = list(map(add, past, grad_accumulator))\n",
        "\n",
        "    return pert_past, new_accumulated_hidden, grad_norms, loss_per_iter\n",
        "\n",
        "\n",
        "def get_classifier(\n",
        "        name: Optional[str],\n",
        "        class_label: Union[str, int],\n",
        "        device: str,\n",
        "        verbosity_level: int = REGULAR\n",
        ") -> Tuple[Optional[ClassificationHead], Optional[int]]:\n",
        "    if name is None:\n",
        "        return None, None\n",
        "\n",
        "    params = DISCRIMINATOR_MODELS_PARAMS[name]\n",
        "    classifier = ClassificationHead(\n",
        "        class_size=params['class_size'],\n",
        "        embed_size=params['embed_size']\n",
        "    ).to(device)\n",
        "    if \"url\" in params:\n",
        "        resolved_archive_file = cached_path(params[\"url\"])\n",
        "    elif \"path\" in params:\n",
        "        resolved_archive_file = params[\"path\"]\n",
        "    else:\n",
        "        raise ValueError(\"Either url or path have to be specified \"\n",
        "                         \"in the discriminator model parameters\")\n",
        "    classifier.load_state_dict(\n",
        "        torch.load(resolved_archive_file, map_location=device))\n",
        "    classifier.eval()\n",
        "\n",
        "    if isinstance(class_label, str):\n",
        "        if class_label in params[\"class_vocab\"]:\n",
        "            label_id = params[\"class_vocab\"][class_label]\n",
        "        else:\n",
        "            label_id = params[\"default_class\"]\n",
        "            if verbosity_level >= REGULAR:\n",
        "                print(\"class_label {} not in class_vocab\".format(class_label))\n",
        "                print(\"available values are: {}\".format(params[\"class_vocab\"]))\n",
        "                print(\"using default class {}\".format(label_id))\n",
        "\n",
        "    elif isinstance(class_label, int):\n",
        "        if class_label in set(params[\"class_vocab\"].values()):\n",
        "            label_id = class_label\n",
        "        else:\n",
        "            label_id = params[\"default_class\"]\n",
        "            if verbosity_level >= REGULAR:\n",
        "                print(\"class_label {} not in class_vocab\".format(class_label))\n",
        "                print(\"available values are: {}\".format(params[\"class_vocab\"]))\n",
        "                print(\"using default class {}\".format(label_id))\n",
        "\n",
        "    else:\n",
        "        label_id = params[\"default_class\"]\n",
        "\n",
        "    return classifier, label_id\n",
        "\n",
        "\n",
        "def get_bag_of_words_indices(bag_of_words_ids_or_paths: List[str], tokenizer) -> \\\n",
        "        List[List[List[int]]]:\n",
        "    bow_indices = []\n",
        "    for id_or_path in bag_of_words_ids_or_paths:\n",
        "        if id_or_path in BAG_OF_WORDS_ARCHIVE_MAP:\n",
        "            filepath = cached_path(BAG_OF_WORDS_ARCHIVE_MAP[id_or_path])\n",
        "        else:\n",
        "            filepath = id_or_path\n",
        "        with open(filepath, \"r\") as f:\n",
        "            words = f.read().strip().split(\"\\n\")\n",
        "        bow_indices.append(\n",
        "            [tokenizer.encode(word.strip(),\n",
        "                              add_prefix_space=True,\n",
        "                              add_special_tokens=False)\n",
        "             for word in words])\n",
        "    return bow_indices\n",
        "\n",
        "def get_affect_words_and_int (affect_class):\n",
        "  emotions = \"https://raw.githubusercontent.com/ishikasingh/Affective-text-gen/master/NRC-Emotion-Intensity-Lexicon-v1.txt\"\n",
        "  filepath = cached_path(emotions)\n",
        "  with open(filepath, \"r\") as f:\n",
        "      words = f.read().strip().split(\"\\n\")[1:]\n",
        "  words = [w.split(\"\\t\") for w in words]\n",
        "  return [w[0] for w in words if w[1] == affect_class], [float(w[-1]) for w in words if w[1] == affect_class]\n",
        "\n",
        "def build_bows_one_hot_vectors(bow_indices, tokenizer, device='cuda'):\n",
        "    if bow_indices is None:\n",
        "        return None\n",
        "\n",
        "    one_hot_bows_vectors = []\n",
        "    for single_bow in bow_indices:\n",
        "        single_bow = list(filter(lambda x: len(x) <= 1, single_bow))\n",
        "\n",
        "        single_bow = torch.tensor(single_bow).to(device)\n",
        "        num_words = single_bow.shape[0]\n",
        "        # print(num_words)\n",
        "        one_hot_bow = torch.zeros(num_words, tokenizer.vocab_size).to(device)\n",
        "        one_hot_bow.scatter_(1, single_bow, 1)\n",
        "        one_hot_bows_vectors.append(one_hot_bow)\n",
        "    return one_hot_bows_vectors\n",
        "\n",
        "def build_bows_one_hot_vectors_aff(bow_indices,affect_int, tokenizer, device='cuda'):\n",
        "    if bow_indices is None or affect_int is None:\n",
        "        return None, None\n",
        "\n",
        "    one_hot_bows_vectors = []\n",
        "    # print(np.array(bow_indices).shape)\n",
        "    for single_bow in bow_indices:\n",
        "        zipped = [[single_bow[i], affect_int[i]] for i in range(len(single_bow))]\n",
        "        single_bow_int = list(filter(lambda x: len(x[0]) <= 1, zipped))\n",
        "        single_bow = [single_bow_int[i][0] for i in range(len(single_bow_int)) ]\n",
        "        affect_ints = [single_bow_int[i][1] for i in range(len(single_bow_int)) ]\n",
        "        # print(single_bow, affect_ints)\n",
        "        # print(len(single_bow), len(affect_ints))\n",
        "        single_bow = torch.tensor(single_bow).to(device)\n",
        "        num_words = single_bow.shape[0]\n",
        "        # print(num_words)\n",
        "        one_hot_bow = torch.zeros(num_words, tokenizer.vocab_size).to(device)\n",
        "        one_hot_bow.scatter_(1, single_bow, 1)\n",
        "        one_hot_bows_vectors.append(one_hot_bow)\n",
        "    return one_hot_bows_vectors, affect_ints\n",
        "\n",
        "\n",
        "\n",
        "def full_text_generation(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        affect_weight=0.2,\n",
        "        knob = None,\n",
        "        context=None,\n",
        "        num_samples=1,\n",
        "        device=\"cuda\",\n",
        "        bag_of_words=None,\n",
        "        bag_of_words_affect=None,\n",
        "        discrim=None,\n",
        "        class_label=None,\n",
        "        length=100,\n",
        "        stepsize=0.02,\n",
        "        temperature=1.0,\n",
        "        top_k=10,\n",
        "        sample=True,\n",
        "        num_iterations=3,\n",
        "        grad_length=10000,\n",
        "        horizon_length=1,\n",
        "        window_length=0,\n",
        "        decay=False,\n",
        "        gamma=1.5,\n",
        "        gm_scale=0.9,\n",
        "        kl_scale=0.01,\n",
        "        verbosity_level=REGULAR,\n",
        "        **kwargs\n",
        "):\n",
        "    classifier, class_id = get_classifier(discrim, class_label, device)\n",
        "    # print(\"bog is here\", bag_of_words)\n",
        "    # print(\"affect is: \", bag_of_words_affect)\n",
        "    bow_indices = []\n",
        "    bow_indices_affect = []\n",
        "    if bag_of_words:\n",
        "      bow_indices = get_bag_of_words_indices(bag_of_words.split(\";\"), tokenizer)\n",
        "    if bag_of_words_affect: \n",
        "      affect_words, affect_int = get_affect_words_and_int(bag_of_words_affect)\n",
        "      bow_indices_affect.append([tokenizer.encode(word.strip(),add_prefix_space=True, add_special_tokens=False)for word in affect_words])\n",
        "    # print(\"aff1\", affect_int)\n",
        "    loss_type = PPLM_BOW\n",
        "    if bag_of_words_affect:\n",
        "      loss_type = BOW_AFFECT\n",
        "\n",
        "    unpert_gen_tok_text, _, _ = generate_text_pplm(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        context=context,\n",
        "        device=device,\n",
        "        length=length,\n",
        "        sample=sample,\n",
        "        perturb=False,\n",
        "        verbosity_level=verbosity_level\n",
        "    )\n",
        "\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    pert_gen_tok_texts = []\n",
        "    discrim_losses = []\n",
        "    losses_in_time = []\n",
        "    print(\"After Perturbation\")\n",
        "    for i in range(num_samples):\n",
        "        pert_gen_tok_text, discrim_loss, loss_in_time = generate_text_pplm(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            affect_weight=affect_weight,\n",
        "            context=context,\n",
        "            device=device,\n",
        "            perturb=True,\n",
        "            bow_indices=bow_indices,\n",
        "            bow_indices_affect=bow_indices_affect,\n",
        "            affect_int = affect_int,\n",
        "            knob = knob,\n",
        "            classifier=classifier,\n",
        "            class_label=class_id,\n",
        "            loss_type=loss_type,\n",
        "            length=length,\n",
        "            stepsize=stepsize,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            sample=sample,\n",
        "            num_iterations=num_iterations,\n",
        "            grad_length=grad_length,\n",
        "            horizon_length=horizon_length,\n",
        "            window_length=window_length,\n",
        "            decay=decay,\n",
        "            gamma=gamma,\n",
        "            gm_scale=gm_scale,\n",
        "            kl_scale=kl_scale,\n",
        "            verbosity_level=verbosity_level\n",
        "        )\n",
        "        pert_gen_tok_texts.append(pert_gen_tok_text)\n",
        "        if classifier is not None:\n",
        "            discrim_losses.append(discrim_loss.data.cpu().numpy())\n",
        "        losses_in_time.append(loss_in_time)\n",
        "\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVFnR5a2oIFj"
      },
      "source": [
        "def get_affect_words_and_int1 (affect_class):\n",
        "  emotions = \"https://raw.githubusercontent.com/ishikasingh/Affective-text-gen/master/NRC-AffectIntensity-Lexicon.txt\"\n",
        "  filepath = cached_path(emotions)\n",
        "  with open(filepath, \"r\") as f:\n",
        "      words = f.read().strip().split(\"\\n\")[37:]\n",
        "  words = [w.split(\"\\t\") for w in words]\n",
        "  return [w[0] for w in words if w[-1] == affect_class], [float(w[1]) for w in words if w[-1] == affect_class]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SprqNon6JuHj"
      },
      "source": [
        "k, j = get_affect_words_and_int (\"anger\")\n",
        "k1, j1 = get_affect_words_and_int1 (\"anger\")\n",
        "\n",
        "# type(torch.matmul(torch.FloatTensor(j[:10]), torch.FloatTensor(j[:10])))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp9rSoAKycDc",
        "outputId": "06eb7a0b-c0c8-44f6-831d-c5851b1cd197",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "j==j1, k==k1\n",
        "k[:5], k1[:5], j[:5], j1[:5]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['outraged', 'brutality', 'hatred', 'hateful', 'terrorize'],\n",
              " ['outraged', 'brutality', 'hatred', 'hateful', 'terrorize'],\n",
              " [0.964, 0.959, 0.953, 0.94, 0.939],\n",
              " [0.964, 0.959, 0.953, 0.94, 0.939])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOG5q_jJ-Cxl"
      },
      "source": [
        "def run_pplm_example(\n",
        "        pretrained_model=\"gpt2-medium\",\n",
        "        cond_text=\"\",\n",
        "        affect_weight=0.2,\n",
        "        knob = None,\n",
        "        uncond=False,\n",
        "        num_samples=1,\n",
        "        bag_of_words=None,\n",
        "        bag_of_words_affect=None,\n",
        "        discrim=None,\n",
        "        discrim_weights=None,\n",
        "        discrim_meta=None,\n",
        "        class_label=-1,\n",
        "        length=100,\n",
        "        stepsize=0.02,\n",
        "        temperature=1.0,\n",
        "        top_k=10,\n",
        "        sample=True,\n",
        "        num_iterations=3,\n",
        "        grad_length=10000,\n",
        "        horizon_length=1,\n",
        "        window_length=0,\n",
        "        decay=False,\n",
        "        gamma=1.5,\n",
        "        gm_scale=0.9,\n",
        "        kl_scale=0.01,\n",
        "        seed=0,\n",
        "        no_cuda=False,\n",
        "        colorama=False,\n",
        "        verbosity='regular'\n",
        "):\n",
        "    # set Random seed\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # set verbosiry\n",
        "    verbosity_level = VERBOSITY_LEVELS.get(verbosity.lower(), REGULAR)\n",
        "\n",
        "    # # set the device\n",
        "    # device = \"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\"\n",
        "\n",
        "    if discrim == 'generic':\n",
        "        set_generic_model_params(discrim_weights, discrim_meta)\n",
        "\n",
        "    if discrim is not None:\n",
        "        discriminator_pretrained_model = DISCRIMINATOR_MODELS_PARAMS[discrim][\n",
        "            \"pretrained_model\"\n",
        "        ]\n",
        "        if pretrained_model != discriminator_pretrained_model:\n",
        "            pretrained_model = discriminator_pretrained_model\n",
        "            if verbosity_level >= REGULAR:\n",
        "                print(\"discrim = {}, pretrained_model set \"\n",
        "                \"to discriminator's = {}\".format(discrim, pretrained_model))\n",
        "\n",
        "    # # load pretrained model\n",
        "    # model = GPT2LMHeadModel.from_pretrained(\n",
        "    #     pretrained_model,\n",
        "    #     output_hidden_states=True\n",
        "    # )\n",
        "    # model.to(device)\n",
        "    # model.eval()\n",
        "\n",
        "    # # load tokenizer\n",
        "    # tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\n",
        "\n",
        "    # Freeze GPT-2 weights\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # figure out conditioning text\n",
        "    if uncond:\n",
        "        tokenized_cond_text = tokenizer.encode([tokenizer.bos_token],add_special_tokens=False)\n",
        "    else:\n",
        "        raw_text = cond_text\n",
        "        while not raw_text:\n",
        "            print(\"Did you forget to add `--cond_text`? \")\n",
        "            raw_text = input(\"Model prompt >>> \")\n",
        "        tokenized_cond_text = tokenizer.encode(tokenizer.bos_token + raw_text,add_special_tokens=False)\n",
        "    print(\"= Prefix of sentence =\")\n",
        "    # generate unperturbed and perturbed texts\n",
        "\n",
        "    # full_text_generation returns:\n",
        "    # unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time\n",
        "    unpert_gen_tok_text, pert_gen_tok_texts, _, _ = full_text_generation(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        affect_weight=affect_weight,\n",
        "        knob = knob,\n",
        "        context=tokenized_cond_text,\n",
        "        device=device,\n",
        "        num_samples=num_samples,\n",
        "        bag_of_words=bag_of_words,\n",
        "        bag_of_words_affect=bag_of_words_affect,\n",
        "        discrim=discrim,\n",
        "        class_label=class_label,\n",
        "        length=length,\n",
        "        stepsize=stepsize,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        sample=sample,\n",
        "        num_iterations=num_iterations,\n",
        "        grad_length=grad_length,\n",
        "        horizon_length=horizon_length,\n",
        "        window_length=window_length,\n",
        "        decay=decay,\n",
        "        gamma=gamma,\n",
        "        gm_scale=gm_scale,\n",
        "        kl_scale=kl_scale,\n",
        "        verbosity_level=verbosity_level\n",
        "    )\n",
        "\n",
        "    # untokenize unperturbed text\n",
        "    unpert_gen_text = tokenizer.decode(unpert_gen_tok_text.tolist()[0])\n",
        "\n",
        "    if verbosity_level >= REGULAR:\n",
        "        print(\"=\" * 80)\n",
        "    print(\"= Unperturbed generated text =\")\n",
        "    print(unpert_gen_text)\n",
        "    print()\n",
        "\n",
        "    generated_texts = []\n",
        "\n",
        "    # iterate through the perturbed texts\n",
        "    for i, pert_gen_tok_text in enumerate(pert_gen_tok_texts):\n",
        "        try:\n",
        "            # untokenize unperturbed text\n",
        "            pert_gen_text = tokenizer.decode(pert_gen_tok_text.tolist()[0])\n",
        "            print(\"= Perturbed generated text {} =\".format(i + 1))\n",
        "            print(pert_gen_text)\n",
        "            print()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # keep the prefix, perturbed seq, original seq for each index\n",
        "        generated_texts.append(\n",
        "            (tokenized_cond_text, pert_gen_tok_text, unpert_gen_tok_text)\n",
        "        )\n",
        "\n",
        "    return"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIdQW59daqIs"
      },
      "source": [
        "def generate_text_pplm(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        affect_weight=0.2,\n",
        "        context=None,\n",
        "        past=None,\n",
        "        device=\"cuda\",\n",
        "        perturb=True,\n",
        "        bow_indices=None,\n",
        "        bow_indices_affect=None,\n",
        "        affect_int = None,\n",
        "        knob = None,\n",
        "        classifier=None,\n",
        "        class_label=None,\n",
        "        loss_type=0,\n",
        "        length=100,\n",
        "        stepsize=0.02,\n",
        "        temperature=1.0,\n",
        "        top_k=10,\n",
        "        sample=True,\n",
        "        num_iterations=3,\n",
        "        grad_length=10000,\n",
        "        horizon_length=1,\n",
        "        window_length=0,\n",
        "        decay=False,\n",
        "        gamma=1.5,\n",
        "        gm_scale=0.9,\n",
        "        kl_scale=0.01,\n",
        "        verbosity_level=REGULAR\n",
        "):\n",
        "    output_so_far = None\n",
        "    if context:\n",
        "        context_t = torch.tensor(context, device=device, dtype=torch.long)\n",
        "        while len(context_t.shape) < 2:\n",
        "            context_t = context_t.unsqueeze(0)\n",
        "        output_so_far = context_t\n",
        "\n",
        "    # collect one hot vectors for bags of words\n",
        "    one_hot_bows_vectors = build_bows_one_hot_vectors(bow_indices, tokenizer, device)\n",
        "    affect_int_orig = affect_int\n",
        "    one_hot_bows_affect, affect_int = build_bows_one_hot_vectors_aff(bow_indices_affect, affect_int, tokenizer, device)\n",
        "#    print(torch.FloatTensor(one_hot_bows_affect).size())\n",
        "    grad_norms = None\n",
        "    last = None\n",
        "    unpert_discrim_loss = 0\n",
        "    loss_in_time = []\n",
        "\n",
        "    if verbosity_level >= VERBOSE:\n",
        "        range_func = trange(length, ascii=True)\n",
        "    else:\n",
        "        range_func = range(length)\n",
        "    count = 0\n",
        "    int_score = 0\n",
        "    for i in range_func:\n",
        "        if count == 3:\n",
        "          break\n",
        "        # Get past/probs for current output, except for last word\n",
        "        # Note that GPT takes 2 inputs: past + current_token\n",
        "\n",
        "        # run model forward to obtain unperturbed\n",
        "        if past is None and output_so_far is not None:\n",
        "            last = output_so_far[:, -1:]\n",
        "            if output_so_far.shape[1] > 1:\n",
        "                _, past, _ = model(output_so_far[:, :-1])\n",
        "\n",
        "        unpert_logits, unpert_past, unpert_all_hidden = model(output_so_far)\n",
        "        unpert_last_hidden = unpert_all_hidden[-1]\n",
        "\n",
        "        # check if we are abowe grad max length\n",
        "        if i >= grad_length:\n",
        "            current_stepsize = stepsize * 0\n",
        "        else:\n",
        "            current_stepsize = stepsize\n",
        "\n",
        "        # modify the past if necessary\n",
        "        if not perturb or num_iterations == 0:\n",
        "            pert_past = past\n",
        "\n",
        "        else:\n",
        "            accumulated_hidden = unpert_last_hidden[:, :-1, :]\n",
        "            accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\n",
        "\n",
        "            if past is not None:\n",
        "                pert_past, _, grad_norms, loss_this_iter = perturb_past(\n",
        "                    past,\n",
        "                    model,\n",
        "                    last,\n",
        "                    affect_weight = affect_weight,\n",
        "                    unpert_past=unpert_past,\n",
        "                    unpert_logits=unpert_logits,\n",
        "                    accumulated_hidden=accumulated_hidden,\n",
        "                    grad_norms=grad_norms,\n",
        "                    stepsize=current_stepsize,\n",
        "                    one_hot_bows_vectors=one_hot_bows_vectors,\n",
        "                    one_hot_bows_affect=one_hot_bows_affect,\n",
        "                    affect_int = affect_int,\n",
        "                    knob = knob,\n",
        "                    classifier=classifier,\n",
        "                    class_label=class_label,\n",
        "                    loss_type=loss_type,\n",
        "                    num_iterations=num_iterations,\n",
        "                    horizon_length=horizon_length,\n",
        "                    window_length=window_length,\n",
        "                    decay=decay,\n",
        "                    gamma=gamma,\n",
        "                    kl_scale=kl_scale,\n",
        "                    device=device,\n",
        "                    verbosity_level=verbosity_level\n",
        "                )\n",
        "                loss_in_time.append(loss_this_iter)\n",
        "            else:\n",
        "                pert_past = past\n",
        "\n",
        "        pert_logits, past, pert_all_hidden = model(last, past=pert_past)\n",
        "        pert_logits = pert_logits[:, -1, :] / temperature  # + SMALL_CONST\n",
        "        pert_probs = F.softmax(pert_logits, dim=-1)\n",
        "\n",
        "        if classifier is not None:\n",
        "            ce_loss = torch.nn.CrossEntropyLoss()\n",
        "            prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\n",
        "            label = torch.tensor([class_label], device=device,\n",
        "                                 dtype=torch.long)\n",
        "            unpert_discrim_loss = ce_loss(prediction, label)\n",
        "            if verbosity_level >= VERBOSE:\n",
        "                print(\n",
        "                    \"unperturbed discrim loss\",\n",
        "                    unpert_discrim_loss.data.cpu().numpy()\n",
        "                )\n",
        "        else:\n",
        "            unpert_discrim_loss = 0\n",
        "\n",
        "        # Fuse the modified model and original model\n",
        "        if perturb:\n",
        "\n",
        "            unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\n",
        "\n",
        "            pert_probs = ((pert_probs ** gm_scale) * (\n",
        "                    unpert_probs ** (1 - gm_scale)))  # + SMALL_CONST\n",
        "            pert_probs = top_k_filter(pert_probs, k=top_k,\n",
        "                                      probs=True)  # + SMALL_CONST\n",
        "\n",
        "            # rescale\n",
        "            if torch.sum(pert_probs) <= 1:\n",
        "                pert_probs = pert_probs / torch.sum(pert_probs)\n",
        "\n",
        "        else:\n",
        "            pert_logits = top_k_filter(pert_logits, k=top_k)  # + SMALL_CONST\n",
        "            pert_probs = F.softmax(pert_logits, dim=-1)\n",
        "\n",
        "        # sample or greedy\n",
        "        if sample:\n",
        "            last = torch.multinomial(pert_probs, num_samples=1)\n",
        "            # print('pert_prob, last ', pert_probs, last)\n",
        "\n",
        "        else:\n",
        "            _, last = torch.topk(pert_probs, k=1, dim=-1)\n",
        "\n",
        "        # update context/output_so_far appending the new token\n",
        "        output_so_far = (\n",
        "            last if output_so_far is None\n",
        "            else torch.cat((output_so_far, last), dim=1)\n",
        "        )\n",
        "        if verbosity_level >= REGULAR:\n",
        "            print(tokenizer.decode(output_so_far.tolist()[0]))\n",
        "        if(tokenizer.decode(output_so_far.tolist()[0])[-1] == '.' ):\n",
        "          count = count+1\n",
        "        if bow_indices_affect is not None and [output_so_far.tolist()[0][-1]] in bow_indices_affect[0]:\n",
        "          int_word = affect_int_orig[bow_indices_affect[0].index([output_so_far.tolist()[0][-1]])]\n",
        "          print(tokenizer.decode(output_so_far.tolist()[0][-1]), int_word)\n",
        "          int_score = int_score + int_word\n",
        "    print(\"int_score: \", int_score)\n",
        "    # print(\"int.. \" , output_so_far.tolist()[0][-1])\n",
        "    return output_so_far, unpert_discrim_loss, loss_in_time\n",
        "\n",
        "\n",
        "def set_generic_model_params(discrim_weights, discrim_meta):\n",
        "    if discrim_weights is None:\n",
        "        raise ValueError('When using a generic discriminator, '\n",
        "                         'discrim_weights need to be specified')\n",
        "    if discrim_meta is None:\n",
        "        raise ValueError('When using a generic discriminator, '\n",
        "                         'discrim_meta need to be specified')\n",
        "\n",
        "    with open(discrim_meta, 'r') as discrim_meta_file:\n",
        "        meta = json.load(discrim_meta_file)\n",
        "    meta['path'] = discrim_weights\n",
        "    DISCRIMINATOR_MODELS_PARAMS['generic'] = meta"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mcJUcOpZB80"
      },
      "source": [
        "# set the device\n",
        "device = \"cuda\" #if torch.cuda.is_available() and not no_cuda else \"cpu\"\n",
        "\n",
        " # load pretrained model\n",
        "pretrained_model=\"gpt2-medium\"\n",
        "model = GPT2LMHeadModel.from_pretrained(\n",
        "    pretrained_model,\n",
        "    output_hidden_states=True\n",
        ")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eNc2QcfQfaj"
      },
      "source": [
        "# Run the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_fvKlS8eFzL"
      },
      "source": [
        "def f(Knob, Prompt, Topic, Affect):\n",
        "    run_pplm_example(\n",
        "          affect_weight=1,  # it is the convergence rate of affect loss, don't change it :-p\n",
        "          knob = Knob, # 0-1, play with it as much as you want\n",
        "          cond_text=Prompt,\n",
        "          num_samples=1,\n",
        "          bag_of_words=Topic,\n",
        "          bag_of_words_affect=Affect,\n",
        "          length=500,\n",
        "          stepsize=0.01,\n",
        "          sample=True,\n",
        "          num_iterations=3,\n",
        "          window_length=5,\n",
        "          gamma=1.5,\n",
        "          gm_scale=0.95,\n",
        "          kl_scale=0.01,\n",
        "          verbosity='quiet'\n",
        "      )\n",
        "def run():\n",
        "    interact_manual(f, Knob=(0,1, 0.1), Prompt=\"There exists\", \\\n",
        "         Topic = ['legal','military','monsters','politics','positive_words', 'religion', 'science','space','technology'], \\\n",
        "         Affect = ['fear', 'joy', 'anger', 'sadness', 'anticipation', 'disgust', 'surprise', 'trust']);"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt3kOp5KLGk_",
        "outputId": "bcdab53f-34f4-4feb-b175-9cbc73d6080c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177,
          "referenced_widgets": [
            "0439cb92092049b4a14c3cf1b636b5c4",
            "167e4351bff74792bea317b34d0156c1",
            "219fb0854b5c4dd195f3630fe7ce607a",
            "47b6d4a4d95a4854887aa45e9446625c",
            "9c952f336a3045da9ce304c8da4f81c5",
            "ea6f75d7dae64f1f88f64ee7a2a75595",
            "5f3865f5942e4f4596059a684566d280",
            "881f6e8cf2cb4518a02e4fe3ae96f260",
            "237ea3c12e9f43489f6b3279f8e0b8a0",
            "ddb813c3880c41ca8db7dc5acf60e696",
            "aba9e6e67ab5489abf867c1ebe855250",
            "1251f7681af64d1695c4fff67f548863",
            "eb5d3837f49045fa809bdfb7fe7236c1",
            "c350528a586b4720820c6b77aa250470",
            "31e5dd27337740148248d2b82a49377a",
            "c7a1f3aeb10949c9899ff47b59cfd1b9",
            "869e4090b8eb4036a4dbb4acc8606ee4",
            "13ded88e4bce422b99a213975c22a679",
            "db3b0ac319c940119df64fdeae233b5e"
          ]
        }
      },
      "source": [
        "run()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0439cb92092049b4a14c3cf1b636b5c4",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "interactive(children=(FloatSlider(value=0.0, description='Knob', max=1.0), Text(value='There exists', descriptâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlzDWG4y-HRH",
        "outputId": "6b288a68-a32b-48e5-ccc8-5251aaeb625b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "topics = ['legal']#,'military','monsters','politics','positive_words', 'religion', 'science','space','technology']\n",
        "affects = ['fear']#, 'anger', 'sadness'] #'fear', \n",
        "knob_vals = [0.8]#,0.5,0.7,1]\n",
        "\n",
        "for topic in topics:\n",
        "  for affect in affects:\n",
        "    for knob in knob_vals:\n",
        "      print(\"topic:\", topic, \", affect:\", affect, \", knob is:\", knob)\n",
        "      run_pplm_example(\n",
        "          affect_weight=1,  # it is the convergence rate of affect loss, don't change it :-p\n",
        "          knob = knob, # 0-1, play with it as much as you want\n",
        "          cond_text=\"It is\",\n",
        "          num_samples=1,\n",
        "          bag_of_words=topic,\n",
        "          bag_of_words_affect=affect,\n",
        "          length=50,\n",
        "          stepsize=0.005, #topic, affect convergence rate\n",
        "          sample=True,\n",
        "          num_iterations=10,\n",
        "          window_length=5,\n",
        "          gamma=1.5,\n",
        "          gm_scale=0.95,\n",
        "          kl_scale=0.01,\n",
        "          verbosity='quiet'\n",
        "      )"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "topic: legal , affect: fear , knob is: 0.8\n",
            "= Prefix of sentence =\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/modeling_gpt2.py:746: FutureWarning: The `past` argument is deprecated and will be removed in a future version, use `past_key_values` instead.\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "int_score:  0\n",
            "After Perturbation\n",
            " shooting 0.812\n",
            " rampage 0.781\n",
            " killing 0.906\n",
            " violence 0.812\n",
            " shot 0.734\n",
            "int_score:  4.045\n",
            "= Unperturbed generated text =\n",
            "<|endoftext|>It is no secret that the United States is the most economically powerful nation in the world. However, it's important to realize how much more powerful we are when we consider our economic growth.\n",
            "\n",
            "The United States has a population that is now greater than the\n",
            "\n",
            "= Perturbed generated text 1 =\n",
            "<|endoftext|>It is the first time in US history that two women from opposing religious sects have been killed in a US shooting rampage.\n",
            "\n",
            "The killing in San Bernardino, California, of 14 people was sparked by a domestic violence dispute between the couple, who were shot dead\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54ZAm_bsTMI8"
      },
      "source": [
        "# Evaluate Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dysnnWMLAoEw"
      },
      "source": [
        "!pip install pytorch_pretrained_bert\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSUvDkuMAYP1"
      },
      "source": [
        "import math\n",
        "import torch\n",
        "from pytorch_pretrained_bert import OpenAIGPTTokenizer, OpenAIGPTModel, OpenAIGPTLMHeadModel\n",
        "# Load pre-trained model (weights)\n",
        "model = OpenAIGPTLMHeadModel.from_pretrained('openai-gpt')\n",
        "model.eval()\n",
        "# Load pre-trained model tokenizer (vocabulary)\n",
        "tokenizer = OpenAIGPTTokenizer.from_pretrained('openai-gpt')\n",
        "\n",
        "\n",
        "# 21.31652459381952, 61.45907380241148, 26.24923942649312"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHkf64i7ikjl"
      },
      "source": [
        "def score(sentence):\n",
        "    tokenize_input = tokenizer.tokenize(sentence)\n",
        "    tensor_input = torch.tensor([tokenizer.convert_tokens_to_ids(tokenize_input)])\n",
        "    loss=model(tensor_input, lm_labels=tensor_input)\n",
        "    # print(sentence, tokenize_input, tensor_input, loss)\n",
        "    return math.exp(loss)\n",
        "    \n",
        "# use your sentences that you want to evaluate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9XprH-UBBu-"
      },
      "source": [
        "a=['i am a girl . you are a boy .',\n",
        "                'there is a plane on the desk',\n",
        "                        \"there 's a pen on the desk\", \"this is me <eos>\", \"hi i want... i want your book\", \"there is is some noise\"]\n",
        "print([score(i) for i in a])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDGq-DLdBHjz"
      },
      "source": [
        "import pandas as pd\n",
        "data = []\n",
        "file_object = open('AffectLM_GenData.txt', 'r')\n",
        "for line in file_object:\n",
        "  data.append(line[:-1].replace('<eos>', '.').split(' #### '))\n",
        "  data[-1].append(score(data[-1][-1]))\n",
        "file_object.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-IkWREkduLf"
      },
      "source": [
        "line = []\n",
        "with open('pplm.txt','r') as f:\n",
        "  for l in f.readlines():\n",
        "    l = l.split('\\t')\n",
        "    l[-1] = l[-1][:-1]\n",
        "    line.append(l)\n",
        "    \n",
        "line[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sc-dClttEAn"
      },
      "source": [
        "for i in range(len(line)):\n",
        "  line[i].append(score(line[i][2]))\n",
        "line[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRDNXNPZsv1w"
      },
      "source": [
        "inten = [[] for i in range(6)]\n",
        "for i in line:\n",
        "  # if i[0] not in ['The book', 'The robots','The country', 'The issue focused on', 'The relationship','The road']:# and i[1] in task1emos:\n",
        "  if i[0] not in emos:\n",
        "    emos.append(i[1])\n",
        "  if i[1] =='0.01':\n",
        "    inten[0].append(i[3])\n",
        "  if i[1] =='0.02':\n",
        "    inten[1].append(i[3])\n",
        "  if i[1] =='0.05':\n",
        "    inten[2].append(i[3])\n",
        "  if i[1] =='0.1':\n",
        "    inten[3].append(i[3])\n",
        "  if i[1] =='0.5':\n",
        "    inten[4].append(i[3])\n",
        "  if i[1] =='1':\n",
        "    inten[5].append(i[3])\n",
        "\n",
        "\n",
        "[[inten.index(i),sum(i)/len(i)] for i in inten]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMlcqInDfknR"
      },
      "source": [
        "import pickle\n",
        "with open('affLMdata_eval', 'wb') as f:\n",
        "  pickle.dump(data, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6oIiOjofrH3"
      },
      "source": [
        "import pickle\n",
        "with open('affLMdata_eval', 'rb') as f:\n",
        "  data = pickle.load(f)\n",
        "data[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiC44n9mv1SG"
      },
      "source": [
        "data[10:14]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZzT936UjIRP"
      },
      "source": [
        "inten = [[] for i in range(6)]\n",
        "task1emos = ['joy', 'sadness', 'anger', 'posemo', 'sad', 'angry']\n",
        "prompts = []\n",
        "emo = []\n",
        "for i in data:\n",
        "  if i[0] not in ['The book', 'The robots','The country', 'The issue focused on', 'The relationship','The road', 'the book', 'the robots','the country', 'the issue focused on', 'the relationship','the road'] and i[1] in task1emos:\n",
        "    if i[0] not in prompts:\n",
        "      prompts.append(i[0])\n",
        "    if i[1] not in emo:\n",
        "      emo.append(i[1])\n",
        "    if i[2] =='0.0':\n",
        "      inten[0].append(i[4])\n",
        "    if i[2] =='1.0':\n",
        "      inten[1].append(i[4])\n",
        "    if i[2] =='2.0':\n",
        "      inten[2].append(i[4])\n",
        "    if i[2] =='3.0':\n",
        "      inten[3].append(i[4])\n",
        "    if i[2] =='4.0':\n",
        "      inten[4].append(i[4])\n",
        "    if i[2] =='5.0':\n",
        "      inten[5].append(i[4])\n",
        "\n",
        "\n",
        "[[inten.index(i),sum(i)/len(i)] for i in inten], emo, prompts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JuVEC1TujGBo"
      },
      "source": [
        "pr = []\n",
        "for i in data:\n",
        "  if i[0] not in pr:\n",
        "    pr.append(i[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGPzMK6siUm4"
      },
      "source": [
        "[[] for i in range(2)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpStDMNAaMg4"
      },
      "source": [
        "import pickle\n",
        "with open('pplm_sentences_scores.jpg', 'rb') as f:\n",
        "  data = pickle.load(f)\n",
        "data[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-ohblimabds"
      },
      "source": [
        "# !touch pplm.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaVxnsFxUy3K"
      },
      "source": [
        "# Other Plottings and p-value Evaluations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VBpE3cHbdvR"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV-OvCCubnlj"
      },
      "source": [
        "%cd /content/gdrive/My\\ Drive/NLP\\ |\\ AffTextGen\\ |\\ Human Eval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IReMECDvcAQM"
      },
      "source": [
        "def get_affect_words_and_int (affect_class):\n",
        "  emotions = \"https://raw.githubusercontent.com/ishikasingh/Affective-text-gen/master/NRC-Emotion-Intensity-Lexicon-v1.txt\"\n",
        "  filepath = cached_path(emotions)\n",
        "  with open(filepath, \"r\") as f:\n",
        "      words = f.read().strip().split(\"\\n\")[1:]\n",
        "  words = [w.split(\"\\t\") for w in words]\n",
        "  return [w[0] for w in words if w[1] == affect_class], [float(w[-1]) for w in words if w[1] == affect_class]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYlGvPKF2K6t"
      },
      "source": [
        "joy = get_affect_words_and_int('joy')\n",
        "anger = get_affect_words_and_int('anger')\n",
        "sad = get_affect_words_and_int('sadness')\n",
        "fear = get_affect_words_and_int('fear')\n",
        "anticipation = get_affect_words_and_int('anticipation')\n",
        "disgust = get_affect_words_and_int('disgust')\n",
        "surprise = get_affect_words_and_int('surprise')\n",
        "trust = get_affect_words_and_int('trust')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m2Ha6Bv3KOp"
      },
      "source": [
        "i=11\n",
        "j = [joy[1][i] for i in range(len(joy[0])) if i%100==0]\n",
        "j1 = [joy[0][i] for i in range(len(joy[0])) if i%100==0]\n",
        "a = [anger[1][i] for i in range(len(anger[0])) if i%(int(len(anger[0])/12))==0]\n",
        "a1 = [anger[0][i] for i in range(len(anger[0])) if i%(int(len(anger[0])/12))==0]\n",
        "s = [sad[1][i] for i in range(len(sad[0])) if i%(int(len(sad[0])/12))==0]\n",
        "s1 = [sad[0][i] for i in range(len(sad[0])) if i%(int(len(sad[0])/12))==0]\n",
        "f = [fear[1][i] for i in range(len(fear[0])) if i%(int(len(fear[0])/12))==0]\n",
        "f1 = [fear[0][i] for i in range(len(fear[0])) if i%(int(len(fear[0])/12))==0]\n",
        "an = [anticipation[1][i] for i in range(len(anticipation[0])) if i%(int(len(anticipation[0])/12))==0]\n",
        "an1 = [anticipation[0][i] for i in range(len(anticipation[0])) if i%(int(len(anticipation[0])/12))==0]\n",
        "d = [disgust[1][i] for i in range(len(disgust[0])) if i%(int(len(disgust[0])/12))==0 and disgust[0][i]!='pornography']\n",
        "d1 = [disgust[0][i] for i in range(len(disgust[0])) if i%(int(len(disgust[0])/12))==0 and disgust[0][i]!='pornography']\n",
        "su = [surprise[1][i] for i in range(len(surprise[0])) if i%(int(len(surprise[0])/12))==0]\n",
        "su1 = [surprise[0][i] for i in range(len(surprise[0])) if i%(int(len(surprise[0])/12))==0]\n",
        "t = [trust[1][i] for i in range(len(trust[0])) if i%(int(len(trust[0])/12))==0]\n",
        "t1 = [trust[0][i] for i in range(len(trust[0])) if i%(int(len(trust[0])/12))==0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ga5EAQ8Q3O2g"
      },
      "source": [
        "j"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNxTeMhk4DS4"
      },
      "source": [
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(15, 7))\n",
        "# fig.suptitle('bold figure suptitle', fontsize=14, fontweight='bold')\n",
        "\n",
        "ax = fig.add_subplot(111)\n",
        "fig.subplots_adjust(top=0.99)\n",
        "# fig.subplots_adjust(bottom=-0.9)\n",
        "# ax.set_title('axes title')\n",
        "\n",
        "ax.set_xlabel('Emotion Category',fontsize=12)\n",
        "ax.set_ylabel('Emotion Intensity', fontsize=12)\n",
        "\n",
        "# ax.text(3, 8, 'boxed italics text in data coords', style='italic',\n",
        "        # bbox={'facecolor': 'red', 'alpha': 0.5, 'pad': 10})\n",
        "\n",
        "# ax.text(2, 6, r'an equation: $E=mc^2$', fontsize=15)\n",
        "\n",
        "# ax.text(0.95, 0.01, 'colored text in axes coords',\n",
        "#         verticalalignment='bottom', horizontalalignment='right',\n",
        "#         transform=ax.transAxes,\n",
        "#         color='green', fontsize=15)\n",
        "\n",
        "em=['joy']*len(j)+['fear']*len(f)+ ['anger']*len(a)+ ['sadness']*(len(s)-1)+ ['anticipation']*len(an)+ ['disgust']*len(d)+ ['surprise']*len(su)+ ['trust']*len(t)\n",
        "emnum = em=[0]*len(j)+[1]*len(f)+ [2]*len(a)+ [3]*(len(s)-1)+ [4]*len(an)+ [5]*len(d)+ [6]*len(su)+ [7]*len(t)\n",
        "inten = j+f+a+s[:-1]+an+d+su+t\n",
        "word = j1+f1+a1+s1[:-1]+an1+d1+su1+t1\n",
        "for i in range(len(em)):\n",
        "  ax.text(emnum[i]+0.04, inten[i]-0.004, word[i], fontsize=11)\n",
        "\n",
        "ax.plot(['joy']*len(j) ,j , 'o',color= '#33A7FF')#0000FF)\n",
        "ax.plot(['fear']*len(f) ,f, 'o',color= '#990012')\n",
        "ax.plot(['anger']*len(a) ,a, 'o',color= '#E42217')\n",
        "ax.plot(['sadness']*(len(s)-1) ,s[:-1], 'o',color= '#151B8D')\n",
        "ax.plot(['anticipation']*len(an) ,an, 'o',color= '#FF8040')\n",
        "ax.plot(['disgust']*len(d) ,d, 'o',color= '#8E35EF')\n",
        "ax.plot(['surprise']*len(su) ,su, 'o',color= '#FDD017')\n",
        "ax.plot(['trust']*len(t) ,t, 'o',color= '#4CC417')\n",
        "plt.xticks(fontsize=12)\n",
        "ax.axis([-1, 8, 0, 1])\n",
        "plt.savefig('emotion-word-dist.pdf', format='pdf', dpi=1200)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZmdx4hByr3p"
      },
      "source": [
        "!pip install --upgrade --quiet gspread"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfECnK_6zK1g"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaWqlYva4Guw"
      },
      "source": [
        "worksheet = gc.open('Human_Eval_Reponses').sheet1\n",
        "\n",
        "# get_all_values gives a list of rows.\n",
        "rows = worksheet.get_all_values()\n",
        "print(rows)\n",
        "\n",
        "import pandas as pd\n",
        "task1 = pd.DataFrame.from_records(rows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PevjyF7K1KCP"
      },
      "source": [
        "task1_aff = task1[:36]\n",
        "task1_our = task1.iloc[36:]\n",
        "task1_aff.iloc[-1], task1_our.iloc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwgFcr0h29s4"
      },
      "source": [
        "task1_aff.iloc[10][2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6chMMctY4CZR"
      },
      "source": [
        "len(task1_our)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUEPreovyp2D"
      },
      "source": [
        "for j in range(3,8):\n",
        "  a1=0; a2 =0; a3 = 0; a1c=0; a2c=0; a3c=0\n",
        "  for i in range(len(task1_aff)):\n",
        "    if task1_aff.iloc[i][2]=='1':\n",
        "      a1+=1\n",
        "      if task1_aff.iloc[i][1] == task1_aff.iloc[i][j]:\n",
        "        a1c+=1\n",
        "    if task1_aff.iloc[i][2]=='2':\n",
        "      a2+=1\n",
        "      if task1_aff.iloc[i][1] == task1_aff.iloc[i][j]:\n",
        "        a2c+=1\n",
        "    if task1_aff.iloc[i][2]=='3':\n",
        "      a3+=1\n",
        "      if task1_aff.iloc[i][1] == task1_aff.iloc[i][j]:\n",
        "        a3c+=1\n",
        "  print(a1c/a1, a2c/a2, a3c/a3, a1c,a1, a2c,a2, a3c,a3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZPoK4rD0eA8"
      },
      "source": [
        "for j in range(3,8):\n",
        "  a1=0; a2 =0; a3 = 0; a1c=0; a2c=0; a3c=0\n",
        "  for i in range(len(task1_our)):\n",
        "    if task1_our.iloc[i][2]=='0.4':\n",
        "      a1+=1\n",
        "      if task1_our.iloc[i][1] == task1_our.iloc[i][j]:\n",
        "        a1c+=1\n",
        "    if task1_our.iloc[i][2]=='0.6':\n",
        "      a2+=1\n",
        "      if task1_our.iloc[i][1] == task1_our.iloc[i][j]:\n",
        "        a2c+=1\n",
        "    if task1_our.iloc[i][2]=='1':\n",
        "      a3+=1\n",
        "      if task1_our.iloc[i][1] == task1_our.iloc[i][j]:\n",
        "        a3c+=1\n",
        "  print(a1c/a1, a2c/a2, a3c/a3, a1c,a1, a2c,a2, a3c,a3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kphjxrlz-bbN"
      },
      "source": [
        "task2[:7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsR-MyLN6Ghv"
      },
      "source": [
        "for j in range(2,6):\n",
        "  a1=0; a2 =0; a3 = 0; a1c=0; a2c=0; a3c=0\n",
        "  for i in range(6, len(task2)):\n",
        "    # print(task2.iloc[i])\n",
        "    if task2.iloc[i][1]=='0.4':\n",
        "      a1+=1\n",
        "      if task2.iloc[i][0] == task2.iloc[i][j]:\n",
        "        a1c+=1\n",
        "    if task2.iloc[i][1]=='0.6':\n",
        "      a2+=1\n",
        "      if task2.iloc[i][0] == task2.iloc[i][j]:\n",
        "        a2c+=1\n",
        "    if task2.iloc[i][1]=='1':\n",
        "      a3+=1\n",
        "      if task2.iloc[i][0] == task2.iloc[i][j]:\n",
        "        a3c+=1\n",
        "  print(a1c/a1, a2c/a2, a3c/a3, a1c,a1, a2c,a2, a3c,a3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXelDh24fT9n"
      },
      "source": [
        "!pip install pingouin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy2NCb4AffA_"
      },
      "source": [
        "worksheet = gc.open('Human_Eval_Reponses').sheet1\n",
        "\n",
        "# get_all_values gives a list of rows.\n",
        "rows = worksheet.get_all_values()\n",
        "print(rows)\n",
        "\n",
        "import pandas as pd\n",
        "task3 = pd.DataFrame.from_records(rows)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYjpMYOspQAH"
      },
      "source": [
        "task3[:16]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zSDvwtJUoHRN"
      },
      "source": [
        "act_int = list(task3[6:6+234][2])\n",
        "avg = list(task3[6:][8]) ## aditya's-7\n",
        "print(len(act_int), len(avg))\n",
        "i=0; gram=[]; pred_int=[]\n",
        "while i<len(avg):\n",
        "  gram+=avg[i:i+6]\n",
        "  pred_int+=avg[i+6:i+12]\n",
        "  i+=12\n",
        "len(gram), len(pred_int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp_aQ7jAqyVR"
      },
      "source": [
        "act_int = [float(i) for i in act_int]\n",
        "gram = [float(i) for i in gram]\n",
        "pred_int = [float(i)+1 for i in pred_int]\n",
        "act_int[:2], gram[:7], pred_int[:7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKSXFKkK6-SV"
      },
      "source": [
        "([0.0, 0.6],\n",
        " [5.25, 4.5, 4.0, 6.75, 5.75, 6.75, 6.0],\n",
        " [3.0, 3.5, 2.75, 3.5, 2.0, 1.25, 1.25])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIMxhjOuszHH"
      },
      "source": [
        "import pandas as pd\n",
        "import pingouin as pg\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_C9Cp-yrcsJ"
      },
      "source": [
        "i=0; g= [[] for i in range(13)]; p = [[] for i in range(13)]; j=0\n",
        "while i <len(act_int):\n",
        "  data = pd.DataFrame(list(zip(act_int[i:i+18], gram[i:i+18], pred_int[i:i+18])), \n",
        "                columns =[ 'act', 'gram', 'pred_int']) \n",
        "  \n",
        "  data = data.sort_values('act')\n",
        "  g[j].append([np.mean(list(data['gram'])[3*i:3*i+3]) for i in range(6)])\n",
        "  p[j].append([np.mean(list(data['pred_int'])[3*i:3*i+3]) for i in range(6)])\n",
        "  # print('gr', g)\n",
        "  # print('pi', p)\n",
        "  j+=1\n",
        "  i+=18"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFymgsQ0rLMT"
      },
      "source": [
        "g[1][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5F9pKHHbHn0"
      },
      "source": [
        "\n",
        "\n",
        "# ax.plot([1, 2])\n",
        "# ax.set_xlabel('x-label', fontsize=12)\n",
        "# ax.set_ylabel('y-label', fontsize=12)\n",
        "# ax.set_title('Title', fontsize=14)\n",
        "\n",
        "fig2 = plt.figure(constrained_layout=True, figsize=(15, 7))\n",
        "spec2 = gridspec.GridSpec(ncols=3, nrows=2, figure=fig2)\n",
        "\n",
        "f2_ax1 = fig2.add_subplot(spec2[0, 0])\n",
        "f2_ax1.set_xticks([])\n",
        "f2_ax1.set_title('Our Model', fontsize=14)\n",
        "f2_ax1.set_ylabel('Perplexity\\n (Automated Evaluation)\\n (lower is better)', fontsize=12)\n",
        "perp = [32.92,29.19 ,33.49,30.91,29.37,30.85]\n",
        "int_ = [0.0,0.2,0.4,0.6,0.8,1.0]\n",
        "f2_ax1.plot(int_, perp,'o-', label = 'Avg. for 3 emotions')\n",
        "perp= [28.75,29.43,30.09,31.56,30.84,28.58]\n",
        "f2_ax1.plot(int_, perp,'*-', label = 'Avg. for 8 emotions')\n",
        "f2_ax1.legend()\n",
        "f2_ax1.axis([0, 1, 10, 65])\n",
        "f2_ax1.set_xlabel('(a)', fontsize = 12)\n",
        "\n",
        "\n",
        "f2_ax2 = fig2.add_subplot(spec2[0, 1])\n",
        "f2_ax2.set_xticks([])\n",
        "f2_ax2.set_yticks([])\n",
        "f2_ax2.set_title('Affect LM', fontsize=14)\n",
        "perp = [41.01, 36.77,41.84,43.63, 47.86,59.64]\n",
        "int_ = [0,1,2,3,4,5]\n",
        "f2_ax2.plot(int_, perp,'o-', label = 'Avg. for 3 emotions')\n",
        "f2_ax2.axis([0, 5, 10, 65])\n",
        "f2_ax2.legend()\n",
        "f2_ax2.set_xlabel('(b)', fontsize = 12)\n",
        "\n",
        "\n",
        "f2_ax5 = fig2.add_subplot(spec2[0, 2])\n",
        "f2_ax5.set_xticks([])\n",
        "f2_ax5.set_yticks([])\n",
        "perp = [14.38,13.13 ,11.95,15.96,64.47,58.76]\n",
        "int_ =[0.01,0.02,0.05,0.10,0.50,1.00]\n",
        "f2_ax5.plot(int_, perp,'o-', label = 'Avg. for 2 sentiments')\n",
        "f2_ax5.axis([0, 1, 10, 65])\n",
        "f2_ax5.legend()\n",
        "f2_ax5.set_title('PPLM', fontsize=14)\n",
        "f2_ax5.set_xlabel('(c)', fontsize = 12)\n",
        "\n",
        "f2_ax3 = fig2.add_subplot(spec2[1, 0])\n",
        "int_ = [0.0,0.2,0.4,0.6,0.8,1.0]\n",
        "f2_ax3.plot(int_, g[0][0],'o-', label = 'Anger')\n",
        "f2_ax3.plot(int_, g[1][0],'*-', label = 'Sadness')\n",
        "f2_ax3.plot(int_, g[2][0],'+-', label = 'Joy')\n",
        "f2_ax3.plot(int_, g[8][0],'x-', label = 'Fear')\n",
        "f2_ax3.plot(int_, g[9][0],'v-', label = 'Anticipation')\n",
        "f2_ax3.plot(int_, g[10][0],'D-', label = 'Disgust')\n",
        "f2_ax3.plot(int_, g[11][0],'d-', label = 'Surprise')\n",
        "f2_ax3.plot(int_, g[12][0],'^-', label = 'Trust')\n",
        "f2_ax3.legend()\n",
        "f2_ax3.set_ylabel('Grammatical Correctness\\n (Human Evaluation)\\n (higher is better)', fontsize=12)\n",
        "f2_ax3.axis([0, 1, 1, 7])\n",
        "f2_ax3.set_xlabel('(d)\\nEmotion Intensity (Knob)', fontsize = 12)\n",
        "\n",
        "f2_ax4 = fig2.add_subplot(spec2[1, 1])\n",
        "f2_ax4.set_yticks([])\n",
        "int_ = [0,1,2,3,4,5]\n",
        "f2_ax4.plot(int_, g[3][0],'o-', label = 'Anger')\n",
        "f2_ax4.plot(int_, g[4][0],'*-', label = 'Sadness')\n",
        "f2_ax4.plot(int_, g[5][0],'+-', label = 'Positive Emotion')\n",
        "f2_ax4.legend()\n",
        "f2_ax4.axis([0, 5, 1, 7])\n",
        "f2_ax4.set_xlabel('(e)\\nEmotion Intensity (beta)', fontsize = 12)\n",
        "\n",
        "f2_ax6 = fig2.add_subplot(spec2[1, 2])\n",
        "f2_ax6.set_yticks([])\n",
        "int_ =[0.01,0.02,0.05,0.10,0.50,1.00]\n",
        "f2_ax6.plot(int_, g[6][0],'o-', label = 'Negative Sentiment')\n",
        "f2_ax6.plot(int_, g[7][0],'*-', label = 'Positive Sentiment')\n",
        "f2_ax6.legend()\n",
        "f2_ax6.axis([0, 1, 1,7])\n",
        "f2_ax6.set_xlabel('(f)\\nSentiment Intensity (Stepsize)', fontsize = 12)\n",
        "fig2.savefig('grammer_2annot.svg', format='svg', dpi=1200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8KX0l_ANZp4"
      },
      "source": [
        "fig2 = plt.figure(constrained_layout=True, figsize=(15, 5))\n",
        "spec2 = gridspec.GridSpec(ncols=3, nrows=1, figure=fig2)\n",
        "\n",
        "# f2_ax1 = fig2.add_subplot(spec2[0, 0])\n",
        "# f2_ax1.set_xticks([])\n",
        "# f2_ax1.set_title('Our Model', fontsize=14)\n",
        "# f2_ax1.set_ylabel('Perplexity\\n (Automated Evaluation)\\n (lower is better)', fontsize=12)\n",
        "# perp = [32.92,29.19 ,33.49,30.91,29.37,30.85]\n",
        "# int_ = [0.0,0.2,0.4,0.6,0.8,1.0]\n",
        "# f2_ax1.plot(int_, perp,'o-', label = 'Avg. for 3 emotions')\n",
        "# perp= [28.75,29.43,30.09,31.56,30.84,28.58]\n",
        "# f2_ax1.plot(int_, perp,'*-', label = 'Avg. for 8 emotions')\n",
        "# f2_ax1.legend()\n",
        "# f2_ax1.axis([0, 1, 10, 65])\n",
        "# f2_ax1.set_xlabel('(a)', fontsize = 12)\n",
        "\n",
        "\n",
        "# f2_ax2 = fig2.add_subplot(spec2[0, 1])\n",
        "# f2_ax2.set_xticks([])\n",
        "# f2_ax2.set_yticks([])\n",
        "# f2_ax2.set_title('Affect LM', fontsize=14)\n",
        "# perp = [41.01, 36.77,41.84,43.63, 47.86,59.64]\n",
        "# int_ = [0,1,2,3,4,5]\n",
        "# f2_ax2.plot(int_, perp,'o-', label = 'Avg. for 3 emotions')\n",
        "# f2_ax2.axis([0, 5, 10, 65])\n",
        "# f2_ax2.legend()\n",
        "# f2_ax2.set_xlabel('(b)', fontsize = 12)\n",
        "\n",
        "\n",
        "# f2_ax5 = fig2.add_subplot(spec2[0, 2])\n",
        "# f2_ax5.set_xticks([])\n",
        "# f2_ax5.set_yticks([])\n",
        "# perp = [14.38,13.13 ,11.95,15.96,64.47,58.76]\n",
        "# int_ =[0.01,0.02,0.05,0.10,0.50,1.00]\n",
        "# f2_ax5.plot(int_, perp,'o-', label = 'Avg. for 2 sentiments')\n",
        "# f2_ax5.axis([0, 1, 10, 65])\n",
        "# f2_ax5.legend()\n",
        "# f2_ax5.set_title('PPLM', fontsize=14)\n",
        "# f2_ax5.set_xlabel('(c)', fontsize = 12)\n",
        "\n",
        "f2_ax3 = fig2.add_subplot(spec2[0, 0])\n",
        "int_ = [0.0,0.2,0.4,0.6,0.8,1.0]\n",
        "f2_ax3.plot(int_, p[0][0],'o-', label = 'Anger')\n",
        "f2_ax3.plot(int_, p[1][0],'*-', label = 'Sadness')\n",
        "f2_ax3.plot(int_, p[2][0],'+-', label = 'Joy')\n",
        "f2_ax3.plot(int_, p[8][0],'x-', label = 'Fear')\n",
        "f2_ax3.plot(int_, p[9][0],'v-', label = 'Anticipation')\n",
        "f2_ax3.plot(int_, p[10][0],'D-', label = 'Disgust')\n",
        "f2_ax3.plot(int_, p[11][0],'d-', label = 'Surprise')\n",
        "f2_ax3.plot(int_, p[12][0],'^-', label = 'Trust')\n",
        "avg8 = [sum(x)/8 for x in zip(p[0][0], p[1][0],p[2][0],p[8][0],p[9][0],p[10][0],p[11][0],p[12][0])]\n",
        "f2_ax3.plot(int_, avg8,'s-', label = 'Average of 8 emotions', color=\"black\")\n",
        "f2_ax3.legend()\n",
        "f2_ax3.set_title('Our Model', fontsize=14)\n",
        "f2_ax3.set_ylabel('Emotion Intensity Ranking\\n(Human Evaluation)', fontsize=12)\n",
        "f2_ax3.axis([0, 1, 1, 6])\n",
        "f2_ax3.set_xlabel('(a)\\nEmotion Intensity (Knob)', fontsize = 12)\n",
        "\n",
        "f2_ax4 = fig2.add_subplot(spec2[0, 1])\n",
        "f2_ax4.set_yticks([])\n",
        "int_ = [0,1,2,3,4,5]\n",
        "f2_ax4.plot(int_, p[3][0],'o-', label = 'Anger')\n",
        "f2_ax4.plot(int_, p[4][0],'*-', label = 'Sadness')\n",
        "f2_ax4.plot(int_, p[5][0],'+-', label = 'Positive Emotion')\n",
        "avg3 = [sum(x)/3 for x in zip(p[3][0], p[4][0],p[5][0])]\n",
        "f2_ax4.plot(int_, avg3,'s-', label = 'Average of 3 emotions', color=\"black\")\n",
        "f2_ax4.legend()\n",
        "f2_ax4.axis([0, 5, 1, 6])\n",
        "f2_ax4.set_title('Affect LM', fontsize=14)\n",
        "f2_ax4.set_xlabel('(b)\\nEmotion Intensity (beta)', fontsize = 12)\n",
        "\n",
        "f2_ax6 = fig2.add_subplot(spec2[0, 2])\n",
        "f2_ax6.set_yticks([])\n",
        "int_ =[0.01,0.02,0.05,0.10,0.50,1.00]\n",
        "f2_ax6.plot(int_, p[6][0],'o-', label = 'Negative Sentiment')\n",
        "f2_ax6.plot(int_, p[7][0],'*-', label = 'Positive Sentiment')\n",
        "avg2 = [sum(x)/2 for x in zip(p[6][0], p[7][0])]\n",
        "f2_ax6.plot(int_, avg2,'s-', label = 'Average of 2 sentiments', color=\"black\")\n",
        "f2_ax6.legend()\n",
        "f2_ax6.axis([0, 1, 1,6])\n",
        "f2_ax6.set_title('PPLM', fontsize=14)\n",
        "f2_ax6.set_xlabel('(c)\\nSentiment Intensity (Stepsize)', fontsize = 12)\n",
        "\n",
        "fig2.savefig('pred_intensity_2annot.svg', format='svg', dpi=1200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRWlZbl5Ixq3"
      },
      "source": [
        "fig2 = plt.figure(constrained_layout=True, figsize=(10, 5))\n",
        "spec2 = gridspec.GridSpec(ncols=2, nrows=1, figure=fig2)\n",
        "\n",
        "# f2_ax1 = fig2.add_subplot(spec2[0, 0])\n",
        "# f2_ax1.set_xticks([])\n",
        "# f2_ax1.set_title('Our Model', fontsize=14)\n",
        "# f2_ax1.set_ylabel('Perplexity\\n (Automated Evaluation)\\n (lower is better)', fontsize=12)\n",
        "# perp = [32.92,29.19 ,33.49,30.91,29.37,30.85]\n",
        "# int_ = [0.0,0.2,0.4,0.6,0.8,1.0]\n",
        "# f2_ax1.plot(int_, perp,'o-', label = 'Avg. for 3 emotions')\n",
        "# perp= [28.75,29.43,30.09,31.56,30.84,28.58]\n",
        "# f2_ax1.plot(int_, perp,'*-', label = 'Avg. for 8 emotions')\n",
        "# f2_ax1.legend()\n",
        "# f2_ax1.axis([0, 1, 10, 65])\n",
        "# f2_ax1.set_xlabel('(a)', fontsize = 12)\n",
        "\n",
        "\n",
        "# f2_ax2 = fig2.add_subplot(spec2[0, 1])\n",
        "# f2_ax2.set_xticks([])\n",
        "# f2_ax2.set_yticks([])\n",
        "# f2_ax2.set_title('Affect LM', fontsize=14)\n",
        "# perp = [41.01, 36.77,41.84,43.63, 47.86,59.64]\n",
        "# int_ = [0,1,2,3,4,5]\n",
        "# f2_ax2.plot(int_, perp,'o-', label = 'Avg. for 3 emotions')\n",
        "# f2_ax2.axis([0, 5, 10, 65])\n",
        "# f2_ax2.legend()\n",
        "# f2_ax2.set_xlabel('(b)', fontsize = 12)\n",
        "\n",
        "\n",
        "# f2_ax5 = fig2.add_subplot(spec2[0, 2])\n",
        "# f2_ax5.set_xticks([])\n",
        "# f2_ax5.set_yticks([])\n",
        "# perp = [14.38,13.13 ,11.95,15.96,64.47,58.76]\n",
        "# int_ =[0.01,0.02,0.05,0.10,0.50,1.00]\n",
        "# f2_ax5.plot(int_, perp,'o-', label = 'Avg. for 2 sentiments')\n",
        "# f2_ax5.axis([0, 1, 10, 65])\n",
        "# f2_ax5.legend()\n",
        "# f2_ax5.set_title('PPLM', fontsize=14)\n",
        "# f2_ax5.set_xlabel('(c)', fontsize = 12)\n",
        "\n",
        "f2_ax3 = fig2.add_subplot(spec2[0, 0])\n",
        "int_ = [0.0,0.2,0.4,0.6,0.8,1.0]\n",
        "Ang = [0.451, 0.465, 0.472, 0.473, 0.472, 0.463]\n",
        "joy = [0.403, 0.409, 0.405, 0.425, 0.426, 0.412]\n",
        "sad = [0.421, 0.428, 0.428, 0.433, 0.433, 0.420]\n",
        "f2_ax3.plot(int_, Ang,'o-', label = 'Anger')\n",
        "f2_ax3.plot(int_, sad,'*-', label = 'Sadness')\n",
        "f2_ax3.plot(int_, joy,'+-', label = 'Joy')\n",
        "f2_ax3.legend()\n",
        "f2_ax3.set_ylabel('Emotion Intensity Rating\\n(Automated Evaluation)', fontsize=12)\n",
        "f2_ax3.axis([0, 1, 0.4, 0.5])\n",
        "f2_ax3.set_title('Our Model', fontsize=14)\n",
        "f2_ax3.set_xlabel('(a)\\nEmotion Intensity (Knob)', fontsize = 12)\n",
        "\n",
        "f2_ax4 = fig2.add_subplot(spec2[0, 1])\n",
        "# f2_ax4.set_yticks([])\n",
        "int_ = [0,1,2,3,4,5]\n",
        "pos_em=[0.446, 0.458, 0.501, 0.572, 0.638, 0.621]\n",
        "sad= [0.403, 0.417, 0.419, 0.431, 0.435, 0.446]\n",
        "anger = [0.429, 0.435, 0.479, 0.525, 0.557, 0.579]\n",
        "f2_ax4.plot(int_, anger,'o-', label = 'Anger')\n",
        "f2_ax4.plot(int_, sad,'*-', label = 'Sadness')\n",
        "f2_ax4.plot(int_, pos_em,'+-', label = 'Positive Emotion')\n",
        "f2_ax4.legend()\n",
        "f2_ax4.set_title('Affect LM', fontsize=14)\n",
        "f2_ax4.axis([0, 5, 0.4,0.7])\n",
        "f2_ax4.set_xlabel('(b)\\nEmotion Intensity (beta)', fontsize = 12)\n",
        "\n",
        "# f2_ax6 = fig2.add_subplot(spec2[0, 2])\n",
        "# f2_ax6.set_yticks([])\n",
        "# int_ =[0.01,0.02,0.05,0.10,0.50,1.00]\n",
        "# f2_ax6.plot(int_, p[6][0],'o-', label = 'Negative Sentiment')\n",
        "# f2_ax6.plot(int_, p[7][0],'*-', label = 'Positive Sentiment')\n",
        "# f2_ax6.legend()\n",
        "# f2_ax6.axis([0, 1, 1,6])\n",
        "# f2_ax6.set_xlabel('(c)\\nSentiment Intensity (Stepsize)', fontsize = 12)\n",
        "fig2.savefig('pred_intensity_auto.svg', format='svg', dpi=1200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QdQpdWn5i23K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDpkJYgU-6vQ"
      },
      "source": [
        "i=0; m = np.zeros(18); a =0\n",
        "while i <len(act_int):\n",
        "  \n",
        "  data = pd.DataFrame(list(zip(act_int[i:i+18], gram[i:i+18])), \n",
        "                columns =[ 'group', 'weight']) \n",
        "  # print(data)\n",
        "  # data = \"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/PlantGrowth.csv\"\n",
        "\n",
        "  # df = pd.read_csv(data, index_col=0)\n",
        "  # data = data.sort_values('group')\n",
        "  # we = [np.mean(list(data['weight'])[3*i:3*i+3]) for i in range(6)]\n",
        "  # gr = [np.mean(list(data['group'])[3*i:3*i+3]) for i in range(6)]\n",
        "  # data1 = pd.DataFrame(list(zip(gr, we)), \n",
        "  #               columns =[ 'group', 'weight'])\n",
        "  # if a in [0,1,2,8,9,10,11,12]:\n",
        "  #   print(a)\n",
        "  #   m+=np.array(data['weight'])\n",
        "  aov = pg.anova(data=data, dv='weight', between='group', detailed=True)\n",
        "  print(aov)\n",
        "  # pt = pg.pairwise_tukey(dv='weight', between='group', data=data)\n",
        "  # print(pt)\n",
        "  i+=18\n",
        "  a+=1\n",
        "data = pd.DataFrame(list(zip(act_int[:18], list(m))), \n",
        "                columns =[ 'group', 'weight'])\n",
        "aov = pg.anova(data=data, dv='weight', between='group', detailed=True)\n",
        "print(aov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOH_O4nti4vS"
      },
      "source": [
        "0.317, 0.010, 0.048, 0.002, 0.000, 0.0167, 0.352, 0.415, 0.639, 0.203, 0.000, 0.188, 0.067\n",
        "0.451, 0.146, 0.330, 0.553, -, 1, 0.135, 0.008, 0.749, 0.758, 0.692, 0.625, 0.671"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn8Ew_SxwA7R"
      },
      "source": [
        "from statsmodels.multivariate.manova import MANOVA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFE7zFRqfDS9"
      },
      "source": [
        "i=0\n",
        "while i <len(act_int):\n",
        "  data = pd.DataFrame(list(zip(act_int[i:i+18], gram[i:i+18], pred_int[i:i+18])), \n",
        "                columns =[ 'group', 'gram', 'pred_int']) \n",
        "  maov = MANOVA.from_formula('gram + pred_int ~ group', data=data)\n",
        "  print(maov.mv_test())\n",
        "  i+=18"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wemKFfidv8r3"
      },
      "source": [
        "!pip install krippendorff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voBiFtgcufvl"
      },
      "source": [
        "import krippendorff\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ANCoI9yuD_s"
      },
      "source": [
        "task3[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9kZ1FnZuGf9"
      },
      "source": [
        "reldata1 = []\n",
        "reldata2 = []\n",
        "for j in range(4):\n",
        "  avg = list(task3[6:][j+4]) ## aditya's-7\n",
        "  print(len(avg))\n",
        "  i=0; gram=[]; pred_int=[]\n",
        "  while i<len(avg):\n",
        "    gram+=avg[i:i+6]\n",
        "    pred_int+=avg[i+6:i+12]\n",
        "    i+=12\n",
        "  gram = [float(i) for i in gram]\n",
        "  pred_int = [float(i)+1 for i in pred_int]\n",
        "  reldata1.append(pred_int)\n",
        "  reldata2.append(gram)\n",
        "print(len(reldata1))\n",
        "reliability_data = np.array(reldata1)\n",
        "reliability_data.reshape(4, 234)\n",
        "reliability_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PwWji5D86FF"
      },
      "source": [
        "reliability_data = np.array(reldata2)\n",
        "reliability_data.reshape(4, 234)\n",
        "reliability_data.shape\n",
        "reliability_data[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXytzHVBzMKZ"
      },
      "source": [
        "reldata = [list(reliability_data[1]), list(reliability_data[3])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTCltF4r80TH"
      },
      "source": [
        "krippendorff.alpha(reldata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMmRz70S-BIK"
      },
      "source": [
        "01 -0.09, 02 -0.13, 03 0.08\n",
        "12 0.47, 13 0.23\n",
        "23 0.13"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHmD5pw60yXh"
      },
      "source": [
        "task1 =task1.replace(['Anger'], 0).replace(['Positive Emotion'], 1).replace(['Neutral'], 2).replace(['Sadness'], 3)\n",
        "task1[-5:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzeRtmM5zvOW"
      },
      "source": [
        "reldata = [list(task1[:72][7]), list(task1[:72][6])]#, list(task1[:72][6]), list(task1[:72][7])], 3,4,6,7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQGZhV7g28_g"
      },
      "source": [
        "krippendorff.alpha(reldata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKhygaN12g5U"
      },
      "source": [
        "task2 =task2.replace(['Anger'], 0).replace(['Joy'], 1).replace(['Fear'], 2).replace(['Sadness'], 3).replace(['Disgust'], 4).replace(['Anticipation'], 5).replace(['Surprise'], 6).replace(['Trust'], 7)\n",
        "task2[:7]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnPD1Ein2995"
      },
      "source": [
        "reldata = [list(task2[6:][4]), list(task2[6:][5])]#, list(task2[6:][4]), list(task2[6:][5])], 2,3,4,5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9OLwMCGtk-H"
      },
      "source": [
        "krippendorff.alpha(reldata)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrEcEKiLubTR"
      },
      "source": [
        "0.804, 0.162 | \n",
        "0.36, 43 0.33, 46 0.54, 47 0.29, 37 0.22, 36 0.41, 67 0.31\n",
        "0.28, 23 0.26, 24 0.40, 25 0.15, 34 0.15, 35 0.42, 45 0.30"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}