{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "my-pplm.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a09eacca70e24c4791a6ac036ca1501c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b3de9febeb6e4de49ad577c0c859f20b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c1ae0e8de3e84c50a215b7f7c9e2252c",
              "IPY_MODEL_57482a541808424caa62ec2ed87e0c21"
            ]
          }
        },
        "b3de9febeb6e4de49ad577c0c859f20b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c1ae0e8de3e84c50a215b7f7c9e2252c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fb4aef1bcf2c43eebbef636d031e1adf",
            "_dom_classes": [],
            "description": "Downloading",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 870,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 870,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_993fc947326a4cfc8a6977b050e941b2"
          }
        },
        "57482a541808424caa62ec2ed87e0c21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eec3bf19733e43e0801b867b3532ef82",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 870/870 [00:00&lt;00:00, 25.5kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3e1e5a9e8136457f93e7343488517627"
          }
        },
        "fb4aef1bcf2c43eebbef636d031e1adf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "993fc947326a4cfc8a6977b050e941b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eec3bf19733e43e0801b867b3532ef82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3e1e5a9e8136457f93e7343488517627": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishikasingh/my_PPLM/blob/master/my_pplm_working.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIEkru9a89ev",
        "colab_type": "code",
        "outputId": "da9229ac-a65e-4918-c3b3-b29b0794aa1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ee/fc/bd726a15ab2c66dc09306689d04da07a3770dad724f0883f0a4bfb745087/transformers-2.4.1-py3-none-any.whl (475kB)\n",
            "\r\u001b[K     |▊                               | 10kB 29.2MB/s eta 0:00:01\r\u001b[K     |█▍                              | 20kB 3.0MB/s eta 0:00:01\r\u001b[K     |██                              | 30kB 4.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |███▍                            | 51kB 3.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 61kB 4.3MB/s eta 0:00:01\r\u001b[K     |████▉                           | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 81kB 5.6MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 92kB 6.2MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 102kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 112kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 122kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 133kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 143kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 153kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 163kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 174kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 184kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 194kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 204kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 215kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 225kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 235kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 245kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 256kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 266kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 276kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 286kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 296kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 307kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 317kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 327kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 337kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 348kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 358kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 368kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 378kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 389kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 399kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 409kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 419kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 430kB 4.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 440kB 4.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 450kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 460kB 4.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 471kB 4.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 481kB 4.8MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.0.11\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/36/7af38d572c935f8e0462ec7b4f7a46d73a2b3b1a938f50a5e8132d5b2dc5/tokenizers-0.0.11-cp36-cp36m-manylinux1_x86_64.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 44kB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 50.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.47)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 54.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.47)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->transformers) (0.15.2)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884629 sha256=ce58549c5ceff0d28900193ac14474e151d3dccd655e1c992606e3900ec171a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: tokenizers, sacremoses, sentencepiece, transformers\n",
            "Successfully installed sacremoses-0.0.38 sentencepiece-0.1.85 tokenizers-0.0.11 transformers-2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09MUYiLt9wta",
        "colab_type": "code",
        "outputId": "138db785-8e2f-413c-995f-77e456cba187",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 63
        }
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "from operator import add\n",
        "from typing import List, Optional, Tuple, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from tqdm import trange\n",
        "from transformers import GPT2Tokenizer\n",
        "from transformers.file_utils import cached_path\n",
        "from transformers.modeling_gpt2 import GPT2LMHeadModel"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1TvKXs09QVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ClassificationHead(torch.nn.Module):\n",
        "    \"\"\"Classification Head for  transformer encoders\"\"\"\n",
        "\n",
        "    def __init__(self, class_size, embed_size):\n",
        "        super(ClassificationHead, self).__init__()\n",
        "        self.class_size = class_size\n",
        "        self.embed_size = embed_size\n",
        "        # self.mlp1 = torch.nn.Linear(embed_size, embed_size)\n",
        "        # self.mlp2 = (torch.nn.Linear(embed_size, class_size))\n",
        "        self.mlp = torch.nn.Linear(embed_size, class_size)\n",
        "\n",
        "    def forward(self, hidden_state):\n",
        "        # hidden_state = F.relu(self.mlp1(hidden_state))\n",
        "        # hidden_state = self.mlp2(hidden_state)\n",
        "        logits = self.mlp(hidden_state)\n",
        "        return logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX4W7SCz94Q9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PPLM_BOW = 1\n",
        "PPLM_DISCRIM = 2\n",
        "PPLM_BOW_DISCRIM = 3\n",
        "SMALL_CONST = 1e-15\n",
        "BIG_CONST = 1e10\n",
        "\n",
        "QUIET = 0\n",
        "REGULAR = 1\n",
        "VERBOSE = 2\n",
        "VERY_VERBOSE = 3\n",
        "VERBOSITY_LEVELS = {\n",
        "    'quiet': QUIET,\n",
        "    'regular': REGULAR,\n",
        "    'verbose': VERBOSE,\n",
        "    'very_verbose': VERY_VERBOSE,\n",
        "}\n",
        "\n",
        "BAG_OF_WORDS_ARCHIVE_MAP = {\n",
        "    'legal': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/legal.txt\",\n",
        "    'military': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/military.txt\",\n",
        "    'monsters': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/monsters.txt\",\n",
        "    'politics': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/politics.txt\",\n",
        "    'positive_words': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/positive_words.txt\",\n",
        "    'religion': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/religion.txt\",\n",
        "    'science': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/science.txt\",\n",
        "    'space': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/space.txt\",\n",
        "    'technology': \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/bow/technology.txt\",\n",
        "}\n",
        "\n",
        "DISCRIMINATOR_MODELS_PARAMS = {\n",
        "    \"clickbait\": {\n",
        "        \"url\": \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/discriminators/clickbait_classifier_head.pt\",\n",
        "        \"class_size\": 2,\n",
        "        \"embed_size\": 1024,\n",
        "        \"class_vocab\": {\"non_clickbait\": 0, \"clickbait\": 1},\n",
        "        \"default_class\": 1,\n",
        "        \"pretrained_model\": \"gpt2-medium\",\n",
        "    },\n",
        "    \"sentiment\": {\n",
        "        \"url\": \"https://s3.amazonaws.com/models.huggingface.co/bert/pplm/discriminators/SST_classifier_head.pt\",\n",
        "        \"class_size\": 5,\n",
        "        \"embed_size\": 1024,\n",
        "        \"class_vocab\": {\"very_positive\": 2, \"very_negative\": 3},\n",
        "        \"default_class\": 3,\n",
        "        \"pretrained_model\": \"gpt2-medium\",\n",
        "    },\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "503RMAOx976u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_var(x, requires_grad=False, volatile=False, device='cuda'):\n",
        "    if torch.cuda.is_available() and device == 'cuda':\n",
        "        x = x.cuda()\n",
        "    elif device != 'cuda':\n",
        "        x = x.to(device)\n",
        "    return Variable(x, requires_grad=requires_grad, volatile=volatile)\n",
        "\n",
        "\n",
        "def top_k_filter(logits, k, probs=False):\n",
        "    \"\"\"\n",
        "    Masks everything but the k top entries as -infinity (1e10).\n",
        "    Used to mask logits such that e^-infinity -> 0 won't contribute to the\n",
        "    sum of the denominator.\n",
        "    \"\"\"\n",
        "    if k == 0:\n",
        "        return logits\n",
        "    else:\n",
        "        values = torch.topk(logits, k)[0]\n",
        "        batch_mins = values[:, -1].view(-1, 1).expand_as(logits)\n",
        "        if probs:\n",
        "            return torch.where(logits < batch_mins,\n",
        "                               torch.ones_like(logits) * 0.0, logits)\n",
        "        return torch.where(logits < batch_mins,\n",
        "                           torch.ones_like(logits) * -BIG_CONST,\n",
        "                           logits)\n",
        "\n",
        "\n",
        "def perturb_past(\n",
        "        past,\n",
        "        model,\n",
        "        last,\n",
        "        unpert_past=None,\n",
        "        unpert_logits=None,\n",
        "        accumulated_hidden=None,\n",
        "        grad_norms=None,\n",
        "        stepsize=0.01,\n",
        "        one_hot_bows_vectors=None,\n",
        "        classifier=None,\n",
        "        class_label=None,\n",
        "        loss_type=0,\n",
        "        num_iterations=3,\n",
        "        horizon_length=1,\n",
        "        window_length=0,\n",
        "        decay=False,\n",
        "        gamma=1.5,\n",
        "        kl_scale=0.01,\n",
        "        device='cuda',\n",
        "        verbosity_level=REGULAR\n",
        "):\n",
        "    # Generate inital perturbed past\n",
        "    grad_accumulator = [\n",
        "        (np.zeros(p.shape).astype(\"float32\"))\n",
        "        for p in past\n",
        "    ]\n",
        "\n",
        "    if accumulated_hidden is None:\n",
        "        accumulated_hidden = 0\n",
        "\n",
        "    if decay:\n",
        "        decay_mask = torch.arange(\n",
        "            0.,\n",
        "            1.0 + SMALL_CONST,\n",
        "            1.0 / (window_length)\n",
        "        )[1:]\n",
        "    else:\n",
        "        decay_mask = 1.0\n",
        "\n",
        "    # TODO fix this comment (SUMANTH)\n",
        "    # Generate a mask is gradient perturbated is based on a past window\n",
        "    _, _, _, curr_length, _ = past[0].shape\n",
        "\n",
        "    if curr_length > window_length and window_length > 0:\n",
        "        ones_key_val_shape = (\n",
        "                tuple(past[0].shape[:-2])\n",
        "                + tuple([window_length])\n",
        "                + tuple(past[0].shape[-1:])\n",
        "        )\n",
        "\n",
        "        zeros_key_val_shape = (\n",
        "                tuple(past[0].shape[:-2])\n",
        "                + tuple([curr_length - window_length])\n",
        "                + tuple(past[0].shape[-1:])\n",
        "        )\n",
        "\n",
        "        ones_mask = torch.ones(ones_key_val_shape)\n",
        "        ones_mask = decay_mask * ones_mask.permute(0, 1, 2, 4, 3)\n",
        "        ones_mask = ones_mask.permute(0, 1, 2, 4, 3)\n",
        "\n",
        "        window_mask = torch.cat(\n",
        "            (ones_mask, torch.zeros(zeros_key_val_shape)),\n",
        "            dim=-2\n",
        "        ).to(device)\n",
        "    else:\n",
        "        window_mask = torch.ones_like(past[0]).to(device)\n",
        "\n",
        "    # accumulate perturbations for num_iterations\n",
        "    loss_per_iter = []\n",
        "    new_accumulated_hidden = None\n",
        "    for i in range(num_iterations):\n",
        "        if verbosity_level >= VERBOSE:\n",
        "            print(\"Iteration \", i + 1)\n",
        "        curr_perturbation = [\n",
        "            to_var(torch.from_numpy(p_), requires_grad=True, device=device)\n",
        "            for p_ in grad_accumulator\n",
        "        ]\n",
        "\n",
        "        # Compute hidden using perturbed past\n",
        "        perturbed_past = list(map(add, past, curr_perturbation))\n",
        "        _, _, _, curr_length, _ = curr_perturbation[0].shape\n",
        "        all_logits, _, all_hidden = model(last, past=perturbed_past)\n",
        "        hidden = all_hidden[-1]\n",
        "        new_accumulated_hidden = accumulated_hidden + torch.sum(\n",
        "            hidden,\n",
        "            dim=1\n",
        "        ).detach()\n",
        "        # TODO: Check the layer-norm consistency of this with trained discriminator (Sumanth)\n",
        "        logits = all_logits[:, -1, :]\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        loss = 0.0\n",
        "        loss_list = []\n",
        "        if loss_type == PPLM_BOW or loss_type == PPLM_BOW_DISCRIM:\n",
        "            for one_hot_bow in one_hot_bows_vectors:\n",
        "                bow_logits = torch.mm(probs, torch.t(one_hot_bow))\n",
        "                bow_loss = -torch.log(torch.sum(bow_logits))\n",
        "                loss += bow_loss\n",
        "                loss_list.append(bow_loss)\n",
        "            if verbosity_level >= VERY_VERBOSE:\n",
        "                print(\" pplm_bow_loss:\", loss.data.cpu().numpy())\n",
        "\n",
        "        if loss_type == PPLM_DISCRIM or loss_type == PPLM_BOW_DISCRIM:\n",
        "            ce_loss = torch.nn.CrossEntropyLoss()\n",
        "            # TODO why we need to do this assignment and not just using unpert_past? (Sumanth)\n",
        "            curr_unpert_past = unpert_past\n",
        "            curr_probs = torch.unsqueeze(probs, dim=1)\n",
        "            wte = model.resize_token_embeddings()\n",
        "            for _ in range(horizon_length):\n",
        "                inputs_embeds = torch.matmul(curr_probs, wte.weight.data)\n",
        "                _, curr_unpert_past, curr_all_hidden = model(\n",
        "                    past=curr_unpert_past,\n",
        "                    inputs_embeds=inputs_embeds\n",
        "                )\n",
        "                curr_hidden = curr_all_hidden[-1]\n",
        "                new_accumulated_hidden = new_accumulated_hidden + torch.sum(\n",
        "                    curr_hidden, dim=1)\n",
        "\n",
        "            prediction = classifier(new_accumulated_hidden /\n",
        "                                    (curr_length + 1 + horizon_length))\n",
        "\n",
        "            label = torch.tensor(prediction.shape[0] * [class_label],\n",
        "                                 device=device,\n",
        "                                 dtype=torch.long)\n",
        "            discrim_loss = ce_loss(prediction, label)\n",
        "            if verbosity_level >= VERY_VERBOSE:\n",
        "                print(\" pplm_discrim_loss:\", discrim_loss.data.cpu().numpy())\n",
        "            loss += discrim_loss\n",
        "            loss_list.append(discrim_loss)\n",
        "\n",
        "        kl_loss = 0.0\n",
        "        if kl_scale > 0.0:\n",
        "            unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\n",
        "            unpert_probs = (\n",
        "                    unpert_probs + SMALL_CONST *\n",
        "                    (unpert_probs <= SMALL_CONST).float().to(device).detach()\n",
        "            )\n",
        "            correction = SMALL_CONST * (probs <= SMALL_CONST).float().to(\n",
        "                device).detach()\n",
        "            corrected_probs = probs + correction.detach()\n",
        "            kl_loss = kl_scale * (\n",
        "                (corrected_probs * (corrected_probs / unpert_probs).log()).sum()\n",
        "            )\n",
        "            if verbosity_level >= VERY_VERBOSE:\n",
        "                print(' kl_loss', kl_loss.data.cpu().numpy())\n",
        "            loss += kl_loss\n",
        "\n",
        "        loss_per_iter.append(loss.data.cpu().numpy())\n",
        "        if verbosity_level >= VERBOSE:\n",
        "            print(' pplm_loss', (loss - kl_loss).data.cpu().numpy())\n",
        "\n",
        "        # compute gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # calculate gradient norms\n",
        "        if grad_norms is not None and loss_type == PPLM_BOW:\n",
        "            grad_norms = [\n",
        "                torch.max(grad_norms[index], torch.norm(p_.grad * window_mask))\n",
        "                for index, p_ in enumerate(curr_perturbation)\n",
        "            ]\n",
        "        else:\n",
        "            grad_norms = [\n",
        "                (torch.norm(p_.grad * window_mask) + SMALL_CONST)\n",
        "                for index, p_ in enumerate(curr_perturbation)\n",
        "            ]\n",
        "\n",
        "        # normalize gradients\n",
        "        grad = [\n",
        "            -stepsize *\n",
        "            (p_.grad * window_mask / grad_norms[\n",
        "                index] ** gamma).data.cpu().numpy()\n",
        "            for index, p_ in enumerate(curr_perturbation)\n",
        "        ]\n",
        "\n",
        "        # accumulate gradient\n",
        "        grad_accumulator = list(map(add, grad, grad_accumulator))\n",
        "\n",
        "        # reset gradients, just to make sure\n",
        "        for p_ in curr_perturbation:\n",
        "            p_.grad.data.zero_()\n",
        "\n",
        "        # removing past from the graph\n",
        "        new_past = []\n",
        "        for p_ in past:\n",
        "            new_past.append(p_.detach())\n",
        "        past = new_past\n",
        "\n",
        "    # apply the accumulated perturbations to the past\n",
        "    grad_accumulator = [\n",
        "        to_var(torch.from_numpy(p_), requires_grad=True, device=device)\n",
        "        for p_ in grad_accumulator\n",
        "    ]\n",
        "    pert_past = list(map(add, past, grad_accumulator))\n",
        "\n",
        "    return pert_past, new_accumulated_hidden, grad_norms, loss_per_iter\n",
        "\n",
        "\n",
        "def get_classifier(\n",
        "        name: Optional[str],\n",
        "        class_label: Union[str, int],\n",
        "        device: str,\n",
        "        verbosity_level: int = REGULAR\n",
        ") -> Tuple[Optional[ClassificationHead], Optional[int]]:\n",
        "    if name is None:\n",
        "        return None, None\n",
        "\n",
        "    params = DISCRIMINATOR_MODELS_PARAMS[name]\n",
        "    classifier = ClassificationHead(\n",
        "        class_size=params['class_size'],\n",
        "        embed_size=params['embed_size']\n",
        "    ).to(device)\n",
        "    if \"url\" in params:\n",
        "        resolved_archive_file = cached_path(params[\"url\"])\n",
        "    elif \"path\" in params:\n",
        "        resolved_archive_file = params[\"path\"]\n",
        "    else:\n",
        "        raise ValueError(\"Either url or path have to be specified \"\n",
        "                         \"in the discriminator model parameters\")\n",
        "    classifier.load_state_dict(\n",
        "        torch.load(resolved_archive_file, map_location=device))\n",
        "    classifier.eval()\n",
        "\n",
        "    if isinstance(class_label, str):\n",
        "        if class_label in params[\"class_vocab\"]:\n",
        "            label_id = params[\"class_vocab\"][class_label]\n",
        "        else:\n",
        "            label_id = params[\"default_class\"]\n",
        "            if verbosity_level >= REGULAR:\n",
        "                print(\"class_label {} not in class_vocab\".format(class_label))\n",
        "                print(\"available values are: {}\".format(params[\"class_vocab\"]))\n",
        "                print(\"using default class {}\".format(label_id))\n",
        "\n",
        "    elif isinstance(class_label, int):\n",
        "        if class_label in set(params[\"class_vocab\"].values()):\n",
        "            label_id = class_label\n",
        "        else:\n",
        "            label_id = params[\"default_class\"]\n",
        "            if verbosity_level >= REGULAR:\n",
        "                print(\"class_label {} not in class_vocab\".format(class_label))\n",
        "                print(\"available values are: {}\".format(params[\"class_vocab\"]))\n",
        "                print(\"using default class {}\".format(label_id))\n",
        "\n",
        "    else:\n",
        "        label_id = params[\"default_class\"]\n",
        "\n",
        "    return classifier, label_id\n",
        "\n",
        "\n",
        "def get_bag_of_words_indices(bag_of_words_ids_or_paths: List[str], tokenizer) -> \\\n",
        "        List[List[List[int]]]:\n",
        "    bow_indices = []\n",
        "    for id_or_path in bag_of_words_ids_or_paths:\n",
        "        if id_or_path in BAG_OF_WORDS_ARCHIVE_MAP:\n",
        "            filepath = cached_path(BAG_OF_WORDS_ARCHIVE_MAP[id_or_path])\n",
        "        else:\n",
        "            filepath = id_or_path\n",
        "        with open(filepath, \"r\") as f:\n",
        "            words = f.read().strip().split(\"\\n\")\n",
        "        bow_indices.append(\n",
        "            [tokenizer.encode(word.strip(),\n",
        "                              add_prefix_space=True,\n",
        "                              add_special_tokens=False)\n",
        "             for word in words])\n",
        "    return bow_indices\n",
        "\n",
        "\n",
        "def build_bows_one_hot_vectors(bow_indices, tokenizer, device='cuda'):\n",
        "    if bow_indices is None:\n",
        "        return None\n",
        "\n",
        "    one_hot_bows_vectors = []\n",
        "    for single_bow in bow_indices:\n",
        "        single_bow = list(filter(lambda x: len(x) <= 1, single_bow))\n",
        "        single_bow = torch.tensor(single_bow).to(device)\n",
        "        num_words = single_bow.shape[0]\n",
        "        one_hot_bow = torch.zeros(num_words, tokenizer.vocab_size).to(device)\n",
        "        one_hot_bow.scatter_(1, single_bow, 1)\n",
        "        one_hot_bows_vectors.append(one_hot_bow)\n",
        "    return one_hot_bows_vectors\n",
        "\n",
        "\n",
        "def full_text_generation(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        context=None,\n",
        "        num_samples=1,\n",
        "        device=\"cuda\",\n",
        "        bag_of_words=None,\n",
        "        discrim=None,\n",
        "        class_label=None,\n",
        "        length=100,\n",
        "        stepsize=0.02,\n",
        "        temperature=1.0,\n",
        "        top_k=10,\n",
        "        sample=True,\n",
        "        num_iterations=3,\n",
        "        grad_length=10000,\n",
        "        horizon_length=1,\n",
        "        window_length=0,\n",
        "        decay=False,\n",
        "        gamma=1.5,\n",
        "        gm_scale=0.9,\n",
        "        kl_scale=0.01,\n",
        "        verbosity_level=REGULAR,\n",
        "        **kwargs\n",
        "):\n",
        "    classifier, class_id = get_classifier(discrim, class_label, device)\n",
        "\n",
        "    bow_indices = []\n",
        "    if bag_of_words:\n",
        "        bow_indices = get_bag_of_words_indices(bag_of_words.split(\";\"),\n",
        "                                               tokenizer)\n",
        "\n",
        "    if bag_of_words and classifier:\n",
        "        loss_type = PPLM_BOW_DISCRIM\n",
        "        if verbosity_level >= REGULAR:\n",
        "            print(\"Both PPLM-BoW and PPLM-Discrim are on. \"\n",
        "                  \"This is not optimized.\")\n",
        "\n",
        "    elif bag_of_words:\n",
        "        loss_type = PPLM_BOW\n",
        "        if verbosity_level >= REGULAR:\n",
        "            print(\"Using PPLM-BoW\")\n",
        "\n",
        "    elif classifier is not None:\n",
        "        loss_type = PPLM_DISCRIM\n",
        "        if verbosity_level >= REGULAR:\n",
        "            print(\"Using PPLM-Discrim\")\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"Specify either a bag of words or a discriminator\")\n",
        "\n",
        "    unpert_gen_tok_text, _, _ = generate_text_pplm(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        context=context,\n",
        "        device=device,\n",
        "        length=length,\n",
        "        sample=sample,\n",
        "        perturb=False,\n",
        "        verbosity_level=verbosity_level\n",
        "    )\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    pert_gen_tok_texts = []\n",
        "    discrim_losses = []\n",
        "    losses_in_time = []\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        pert_gen_tok_text, discrim_loss, loss_in_time = generate_text_pplm(\n",
        "            model=model,\n",
        "            tokenizer=tokenizer,\n",
        "            context=context,\n",
        "            device=device,\n",
        "            perturb=True,\n",
        "            bow_indices=bow_indices,\n",
        "            classifier=classifier,\n",
        "            class_label=class_id,\n",
        "            loss_type=loss_type,\n",
        "            length=length,\n",
        "            stepsize=stepsize,\n",
        "            temperature=temperature,\n",
        "            top_k=top_k,\n",
        "            sample=sample,\n",
        "            num_iterations=num_iterations,\n",
        "            grad_length=grad_length,\n",
        "            horizon_length=horizon_length,\n",
        "            window_length=window_length,\n",
        "            decay=decay,\n",
        "            gamma=gamma,\n",
        "            gm_scale=gm_scale,\n",
        "            kl_scale=kl_scale,\n",
        "            verbosity_level=verbosity_level\n",
        "        )\n",
        "        pert_gen_tok_texts.append(pert_gen_tok_text)\n",
        "        if classifier is not None:\n",
        "            discrim_losses.append(discrim_loss.data.cpu().numpy())\n",
        "        losses_in_time.append(loss_in_time)\n",
        "\n",
        "    if device == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    return unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time\n",
        "\n",
        "\n",
        "def generate_text_pplm(\n",
        "        model,\n",
        "        tokenizer,\n",
        "        context=None,\n",
        "        past=None,\n",
        "        device=\"cuda\",\n",
        "        perturb=True,\n",
        "        bow_indices=None,\n",
        "        classifier=None,\n",
        "        class_label=None,\n",
        "        loss_type=0,\n",
        "        length=100,\n",
        "        stepsize=0.02,\n",
        "        temperature=1.0,\n",
        "        top_k=10,\n",
        "        sample=True,\n",
        "        num_iterations=3,\n",
        "        grad_length=10000,\n",
        "        horizon_length=1,\n",
        "        window_length=0,\n",
        "        decay=False,\n",
        "        gamma=1.5,\n",
        "        gm_scale=0.9,\n",
        "        kl_scale=0.01,\n",
        "        verbosity_level=REGULAR\n",
        "):\n",
        "    output_so_far = None\n",
        "    if context:\n",
        "        context_t = torch.tensor(context, device=device, dtype=torch.long)\n",
        "        while len(context_t.shape) < 2:\n",
        "            context_t = context_t.unsqueeze(0)\n",
        "        output_so_far = context_t\n",
        "\n",
        "    # collect one hot vectors for bags of words\n",
        "    one_hot_bows_vectors = build_bows_one_hot_vectors(bow_indices, tokenizer,\n",
        "                                                      device)\n",
        "\n",
        "    grad_norms = None\n",
        "    last = None\n",
        "    unpert_discrim_loss = 0\n",
        "    loss_in_time = []\n",
        "\n",
        "    if verbosity_level >= VERBOSE:\n",
        "        range_func = trange(length, ascii=True)\n",
        "    else:\n",
        "        range_func = range(length)\n",
        "\n",
        "    for i in range_func:\n",
        "\n",
        "        # Get past/probs for current output, except for last word\n",
        "        # Note that GPT takes 2 inputs: past + current_token\n",
        "\n",
        "        # run model forward to obtain unperturbed\n",
        "        if past is None and output_so_far is not None:\n",
        "            last = output_so_far[:, -1:]\n",
        "            if output_so_far.shape[1] > 1:\n",
        "                _, past, _ = model(output_so_far[:, :-1])\n",
        "\n",
        "        unpert_logits, unpert_past, unpert_all_hidden = model(output_so_far)\n",
        "        unpert_last_hidden = unpert_all_hidden[-1]\n",
        "\n",
        "        # check if we are abowe grad max length\n",
        "        if i >= grad_length:\n",
        "            current_stepsize = stepsize * 0\n",
        "        else:\n",
        "            current_stepsize = stepsize\n",
        "\n",
        "        # modify the past if necessary\n",
        "        if not perturb or num_iterations == 0:\n",
        "            pert_past = past\n",
        "\n",
        "        else:\n",
        "            accumulated_hidden = unpert_last_hidden[:, :-1, :]\n",
        "            accumulated_hidden = torch.sum(accumulated_hidden, dim=1)\n",
        "\n",
        "            if past is not None:\n",
        "                pert_past, _, grad_norms, loss_this_iter = perturb_past(\n",
        "                    past,\n",
        "                    model,\n",
        "                    last,\n",
        "                    unpert_past=unpert_past,\n",
        "                    unpert_logits=unpert_logits,\n",
        "                    accumulated_hidden=accumulated_hidden,\n",
        "                    grad_norms=grad_norms,\n",
        "                    stepsize=current_stepsize,\n",
        "                    one_hot_bows_vectors=one_hot_bows_vectors,\n",
        "                    classifier=classifier,\n",
        "                    class_label=class_label,\n",
        "                    loss_type=loss_type,\n",
        "                    num_iterations=num_iterations,\n",
        "                    horizon_length=horizon_length,\n",
        "                    window_length=window_length,\n",
        "                    decay=decay,\n",
        "                    gamma=gamma,\n",
        "                    kl_scale=kl_scale,\n",
        "                    device=device,\n",
        "                    verbosity_level=verbosity_level\n",
        "                )\n",
        "                loss_in_time.append(loss_this_iter)\n",
        "            else:\n",
        "                pert_past = past\n",
        "\n",
        "        pert_logits, past, pert_all_hidden = model(last, past=pert_past)\n",
        "        pert_logits = pert_logits[:, -1, :] / temperature  # + SMALL_CONST\n",
        "        pert_probs = F.softmax(pert_logits, dim=-1)\n",
        "\n",
        "        if classifier is not None:\n",
        "            ce_loss = torch.nn.CrossEntropyLoss()\n",
        "            prediction = classifier(torch.mean(unpert_last_hidden, dim=1))\n",
        "            label = torch.tensor([class_label], device=device,\n",
        "                                 dtype=torch.long)\n",
        "            unpert_discrim_loss = ce_loss(prediction, label)\n",
        "            if verbosity_level >= VERBOSE:\n",
        "                print(\n",
        "                    \"unperturbed discrim loss\",\n",
        "                    unpert_discrim_loss.data.cpu().numpy()\n",
        "                )\n",
        "        else:\n",
        "            unpert_discrim_loss = 0\n",
        "\n",
        "        # Fuse the modified model and original model\n",
        "        if perturb:\n",
        "\n",
        "            unpert_probs = F.softmax(unpert_logits[:, -1, :], dim=-1)\n",
        "\n",
        "            pert_probs = ((pert_probs ** gm_scale) * (\n",
        "                    unpert_probs ** (1 - gm_scale)))  # + SMALL_CONST\n",
        "            pert_probs = top_k_filter(pert_probs, k=top_k,\n",
        "                                      probs=True)  # + SMALL_CONST\n",
        "\n",
        "            # rescale\n",
        "            if torch.sum(pert_probs) <= 1:\n",
        "                pert_probs = pert_probs / torch.sum(pert_probs)\n",
        "\n",
        "        else:\n",
        "            pert_logits = top_k_filter(pert_logits, k=top_k)  # + SMALL_CONST\n",
        "            pert_probs = F.softmax(pert_logits, dim=-1)\n",
        "\n",
        "        # sample or greedy\n",
        "        if sample:\n",
        "            last = torch.multinomial(pert_probs, num_samples=1)\n",
        "\n",
        "        else:\n",
        "            _, last = torch.topk(pert_probs, k=1, dim=-1)\n",
        "\n",
        "        # update context/output_so_far appending the new token\n",
        "        output_so_far = (\n",
        "            last if output_so_far is None\n",
        "            else torch.cat((output_so_far, last), dim=1)\n",
        "        )\n",
        "        if verbosity_level >= REGULAR:\n",
        "            print(tokenizer.decode(output_so_far.tolist()[0]))\n",
        "\n",
        "    return output_so_far, unpert_discrim_loss, loss_in_time\n",
        "\n",
        "\n",
        "def set_generic_model_params(discrim_weights, discrim_meta):\n",
        "    if discrim_weights is None:\n",
        "        raise ValueError('When using a generic discriminator, '\n",
        "                         'discrim_weights need to be specified')\n",
        "    if discrim_meta is None:\n",
        "        raise ValueError('When using a generic discriminator, '\n",
        "                         'discrim_meta need to be specified')\n",
        "\n",
        "    with open(discrim_meta, 'r') as discrim_meta_file:\n",
        "        meta = json.load(discrim_meta_file)\n",
        "    meta['path'] = discrim_weights\n",
        "    DISCRIMINATOR_MODELS_PARAMS['generic'] = meta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOG5q_jJ-Cxl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_pplm_example(\n",
        "        pretrained_model=\"gpt2-medium\",\n",
        "        cond_text=\"\",\n",
        "        uncond=False,\n",
        "        num_samples=1,\n",
        "        bag_of_words=None,\n",
        "        discrim=None,\n",
        "        discrim_weights=None,\n",
        "        discrim_meta=None,\n",
        "        class_label=-1,\n",
        "        length=100,\n",
        "        stepsize=0.02,\n",
        "        temperature=1.0,\n",
        "        top_k=10,\n",
        "        sample=True,\n",
        "        num_iterations=3,\n",
        "        grad_length=10000,\n",
        "        horizon_length=1,\n",
        "        window_length=0,\n",
        "        decay=False,\n",
        "        gamma=1.5,\n",
        "        gm_scale=0.9,\n",
        "        kl_scale=0.01,\n",
        "        seed=0,\n",
        "        no_cuda=False,\n",
        "        colorama=False,\n",
        "        verbosity='regular'\n",
        "):\n",
        "    # set Random seed\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # set verbosiry\n",
        "    verbosity_level = VERBOSITY_LEVELS.get(verbosity.lower(), REGULAR)\n",
        "\n",
        "    # set the device\n",
        "    device = \"cuda\" if torch.cuda.is_available() and not no_cuda else \"cpu\"\n",
        "\n",
        "    if discrim == 'generic':\n",
        "        set_generic_model_params(discrim_weights, discrim_meta)\n",
        "\n",
        "    if discrim is not None:\n",
        "        discriminator_pretrained_model = DISCRIMINATOR_MODELS_PARAMS[discrim][\n",
        "            \"pretrained_model\"\n",
        "        ]\n",
        "        if pretrained_model != discriminator_pretrained_model:\n",
        "            pretrained_model = discriminator_pretrained_model\n",
        "            if verbosity_level >= REGULAR:\n",
        "                print(\"discrim = {}, pretrained_model set \"\n",
        "                \"to discriminator's = {}\".format(discrim, pretrained_model))\n",
        "\n",
        "    # load pretrained model\n",
        "    model = GPT2LMHeadModel.from_pretrained(\n",
        "        pretrained_model,\n",
        "        output_hidden_states=True\n",
        "    )\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # load tokenizer\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model)\n",
        "\n",
        "    # Freeze GPT-2 weights\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # figure out conditioning text\n",
        "    if uncond:\n",
        "        tokenized_cond_text = tokenizer.encode([tokenizer.bos_token],add_special_tokens=False)\n",
        "    else:\n",
        "        raw_text = cond_text\n",
        "        while not raw_text:\n",
        "            print(\"Did you forget to add `--cond_text`? \")\n",
        "            raw_text = input(\"Model prompt >>> \")\n",
        "        tokenized_cond_text = tokenizer.encode(tokenizer.bos_token + raw_text,add_special_tokens=False)\n",
        "    print(\"tokenized_cond_text: \", tokenized_cond_text)\n",
        "    print(\"= Prefix of sentence =\")\n",
        "    print(tokenizer.decode(tokenized_cond_text))\n",
        "    print()\n",
        "    # generate unperturbed and perturbed texts\n",
        "\n",
        "    # full_text_generation returns:\n",
        "    # unpert_gen_tok_text, pert_gen_tok_texts, discrim_losses, losses_in_time\n",
        "    unpert_gen_tok_text, pert_gen_tok_texts, _, _ = full_text_generation(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        context=tokenized_cond_text,\n",
        "        device=device,\n",
        "        num_samples=num_samples,\n",
        "        bag_of_words=bag_of_words,\n",
        "        discrim=discrim,\n",
        "        class_label=class_label,\n",
        "        length=length,\n",
        "        stepsize=stepsize,\n",
        "        temperature=temperature,\n",
        "        top_k=top_k,\n",
        "        sample=sample,\n",
        "        num_iterations=num_iterations,\n",
        "        grad_length=grad_length,\n",
        "        horizon_length=horizon_length,\n",
        "        window_length=window_length,\n",
        "        decay=decay,\n",
        "        gamma=gamma,\n",
        "        gm_scale=gm_scale,\n",
        "        kl_scale=kl_scale,\n",
        "        verbosity_level=verbosity_level\n",
        "    )\n",
        "\n",
        "    # untokenize unperturbed text\n",
        "    unpert_gen_text = tokenizer.decode(unpert_gen_tok_text.tolist()[0])\n",
        "\n",
        "    if verbosity_level >= REGULAR:\n",
        "        print(\"=\" * 80)\n",
        "    print(\"= Unperturbed generated text =\")\n",
        "    print(unpert_gen_text)\n",
        "    print()\n",
        "\n",
        "    generated_texts = []\n",
        "\n",
        "    bow_word_ids = set()\n",
        "    if bag_of_words and colorama:\n",
        "        bow_indices = get_bag_of_words_indices(bag_of_words.split(\";\"),\n",
        "                                               tokenizer)\n",
        "        for single_bow_list in bow_indices:\n",
        "            # filtering all words in the list composed of more than 1 token\n",
        "            filtered = list(filter(lambda x: len(x) <= 1, single_bow_list))\n",
        "            # w[0] because we are sure w has only 1 item because previous fitler\n",
        "            bow_word_ids.update(w[0] for w in filtered)\n",
        "\n",
        "    # iterate through the perturbed texts\n",
        "    for i, pert_gen_tok_text in enumerate(pert_gen_tok_texts):\n",
        "        try:\n",
        "            # untokenize unperturbed text\n",
        "            if colorama:\n",
        "                import colorama\n",
        "\n",
        "                pert_gen_text = ''\n",
        "                for word_id in pert_gen_tok_text.tolist()[0]:\n",
        "                    if word_id in bow_word_ids:\n",
        "                        pert_gen_text += '{}{}{}'.format(\n",
        "                            colorama.Fore.RED,\n",
        "                            tokenizer.decode([word_id]),\n",
        "                            colorama.Style.RESET_ALL\n",
        "                        )\n",
        "                    else:\n",
        "                        pert_gen_text += tokenizer.decode([word_id])\n",
        "            else:\n",
        "                pert_gen_text = tokenizer.decode(pert_gen_tok_text.tolist()[0])\n",
        "\n",
        "            print(\"= Perturbed generated text {} =\".format(i + 1))\n",
        "            print(pert_gen_text)\n",
        "            print()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # keep the prefix, perturbed seq, original seq for each index\n",
        "        generated_texts.append(\n",
        "            (tokenized_cond_text, pert_gen_tok_text, unpert_gen_tok_text)\n",
        "        )\n",
        "\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlzDWG4y-HRH",
        "colab_type": "code",
        "outputId": "113e753a-daf5-40cb-954a-6a57df0cb256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a09eacca70e24c4791a6ac036ca1501c",
            "b3de9febeb6e4de49ad577c0c859f20b",
            "c1ae0e8de3e84c50a215b7f7c9e2252c",
            "57482a541808424caa62ec2ed87e0c21",
            "fb4aef1bcf2c43eebbef636d031e1adf",
            "993fc947326a4cfc8a6977b050e941b2",
            "eec3bf19733e43e0801b867b3532ef82",
            "3e1e5a9e8136457f93e7343488517627"
          ]
        }
      },
      "source": [
        "run_pplm_example(\n",
        "    cond_text=\"once upon\",\n",
        "    num_samples=1,\n",
        "    bag_of_words='science',\n",
        "    length=50,\n",
        "    stepsize=0.03,\n",
        "    sample=True,\n",
        "    num_iterations=3,\n",
        "    window_length=5,\n",
        "    gamma=1.5,\n",
        "    gm_scale=0.95,\n",
        "    kl_scale=0.01,\n",
        "    verbosity='regular'\n",
        ")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenized_cond_text:  [50256, 27078, 2402]\n",
            "= Prefix of sentence =\n",
            "<|endoftext|>once upon\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a09eacca70e24c4791a6ac036ca1501c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Downloading', max=870, style=ProgressStyle(description_width=…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Using PPLM-BoW\n",
            "<|endoftext|>once upon a\n",
            "<|endoftext|>once upon a time\n",
            "<|endoftext|>once upon a time,\n",
            "<|endoftext|>once upon a time, there\n",
            "<|endoftext|>once upon a time, there were\n",
            "<|endoftext|>once upon a time, there were two\n",
            "<|endoftext|>once upon a time, there were two types\n",
            "<|endoftext|>once upon a time, there were two types of\n",
            "<|endoftext|>once upon a time, there were two types of people\n",
            "<|endoftext|>once upon a time, there were two types of people in\n",
            "<|endoftext|>once upon a time, there were two types of people in America\n",
            "<|endoftext|>once upon a time, there were two types of people in America:\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth,\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty.\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich,\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich, as\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich, as we\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich, as we know\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich, as we know today\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich, as we know today,\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich, as we know today, grew\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich, as we know today, grew up\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich, as we know today, grew up to\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich, as we know today, grew up to become\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich, as we know today, grew up to become rich\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich, as we know today, grew up to become rich.\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich, as we know today, grew up to become rich. They\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich, as we know today, grew up to become rich. They were\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich, as we know today, grew up to become rich. They were born\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich, as we know today, grew up to become rich. They were born into\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich, as we know today, grew up to become rich. They were born into wealth\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich, as we know today, grew up to become rich. They were born into wealth,\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich, as we know today, grew up to become rich. They were born into wealth, and\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich, as we know today, grew up to become rich. They were born into wealth, and their\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich, as we know today, grew up to become rich. They were born into wealth, and their children\n",
            "<|endoftext|>once upon a\n",
            "<|endoftext|>once upon a time\n",
            "<|endoftext|>once upon a time,\n",
            "<|endoftext|>once upon a time, there\n",
            "<|endoftext|>once upon a time, there was\n",
            "<|endoftext|>once upon a time, there was a\n",
            "<|endoftext|>once upon a time, there was a game\n",
            "<|endoftext|>once upon a time, there was a game that\n",
            "<|endoftext|>once upon a time, there was a game that had\n",
            "<|endoftext|>once upon a time, there was a game that had a\n",
            "<|endoftext|>once upon a time, there was a game that had a lot\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer,\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it,\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it.\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it.\n",
            "\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it.\n",
            "\n",
            "\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it.\n",
            "\n",
            "This\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it.\n",
            "\n",
            "This is\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it.\n",
            "\n",
            "This is the\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it.\n",
            "\n",
            "This is the game\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it.\n",
            "\n",
            "This is the game of\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it.\n",
            "\n",
            "This is the game of life\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it.\n",
            "\n",
            "This is the game of life.\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it.\n",
            "\n",
            "This is the game of life.\n",
            "\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it.\n",
            "\n",
            "This is the game of life.\n",
            "\n",
            "\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it.\n",
            "\n",
            "This is the game of life.\n",
            "\n",
            "The\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it.\n",
            "\n",
            "This is the game of life.\n",
            "\n",
            "The world\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it.\n",
            "\n",
            "This is the game of life.\n",
            "\n",
            "The world has\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it.\n",
            "\n",
            "This is the game of life.\n",
            "\n",
            "The world has no\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it.\n",
            "\n",
            "This is the game of life.\n",
            "\n",
            "The world has no end\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it.\n",
            "\n",
            "This is the game of life.\n",
            "\n",
            "The world has no end.\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it.\n",
            "\n",
            "This is the game of life.\n",
            "\n",
            "The world has no end.\n",
            "\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it.\n",
            "\n",
            "This is the game of life.\n",
            "\n",
            "The world has no end.\n",
            "\n",
            "\n",
            "================================================================================\n",
            "= Unperturbed generated text =\n",
            "<|endoftext|>once upon a time, there were two types of people in America: those who were born into wealth, and those who were born into poverty. The rich, as we know today, grew up to become rich. They were born into wealth, and their children\n",
            "\n",
            "= Perturbed generated text 1 =\n",
            "<|endoftext|>once upon a time, there was a game that had a lot to offer, and now that you're able to explore it, it's time to learn it.\n",
            "\n",
            "This is the game of life.\n",
            "\n",
            "The world has no end.\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq56MGW_S0ko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}